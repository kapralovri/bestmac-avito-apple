#!/usr/bin/env python3
""" 
Hot Deals Scanner v2 (Avito)
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç curl_cffi –¥–ª—è –æ–±—Ö–æ–¥–∞ TLS-–±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ –∏ API —Å–º–µ–Ω—ã IP –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏.
"""
import json
import os
import re
import time
import random
import logging
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Optional, List, Set, Dict
from pathlib import Path
from urllib.parse import urljoin

# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
try:
    from curl_cffi import requests as cffi_requests
    HAS_CFFI = True
except ImportError:
    import requests as cffi_requests
    HAS_CFFI = False

from bs4 import BeautifulSoup

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("AvitoScanner")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
DEFAULT_SCAN_URL = "https://www.avito.ru/moskva_i_mo/noutbuki/apple/b_u-ASgBAgICAkTwvA2I0jSo5A302WY?cd=1&f=ASgBAQICAkTwvA2I0jSo5A302WYBQJ7kDdTIn7YVvLGeFajjlxXCmZYVsNjvEdTY7xGc2O8RsqPEEZKjxBGOza0QmM2tEKaaxhDWzK0Q&localPriority=1&q=macbook&s=104"
SCAN_URL = os.environ.get('SCAN_URL', DEFAULT_SCAN_URL)
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
PROXY_URL = os.environ.get('PROXY_URL')
# –ò—Å–ø–æ–ª—å–∑—É–µ–º CHANGE_IP_URL –∏–∑ –≤–∞—à–∏—Ö —Å–µ–∫—Ä–µ—Ç–æ–≤ GitHub
CHANGE_IP_URL = os.environ.get('CHANGE_IP_URL') 

HOT_DEAL_THRESHOLD = 0.90  # 10% –∏ –±–æ–ª–µ–µ –Ω–∏–∂–µ –º–µ–¥–∏–∞–Ω—ã
PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_DEALS_FILE = Path("public/data/seen-hot-deals.json")

IMPERSONATE = "chrome120"
TIMEOUT = 30
MAX_RETRIES = 3

@dataclass
class HotDeal:
    url: str
    title: str
    price: int
    median_price: int
    discount_percent: float
    model: str
    found_at: str

class AvitoScanner:
    def __init__(self):
        self.session = cffi_requests.Session()
        self.prices_db = self._load_prices()
        self.seen_deals = self._load_seen()
        
    def _load_prices(self) -> Dict[str, int]:
        if not PRICES_FILE.exists():
            logger.warning(f"‚ö†Ô∏è –ë–∞–∑–∞ —Ü–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {PRICES_FILE}")
            return {}
        try:
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            prices = {stat['model_name']: stat['median_price'] 
                      for stat in data.get('stats', []) 
                      if 'model_name' in stat and 'median_price' in stat}
            return prices
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ü–µ–Ω: {e}")
            return {}

    def _load_seen(self) -> Set[str]:
        if not SEEN_DEALS_FILE.exists():
            return set()
        try:
            with open(SEEN_DEALS_FILE, 'r', encoding='utf-8') as f:
                return set(json.load(f).get('seen_urls', []))
        except Exception:
            return set()

    def _save_seen(self):
        try:
            SEEN_DEALS_FILE.parent.mkdir(parents=True, exist_ok=True)
            with open(SEEN_DEALS_FILE, 'w', encoding='utf-8') as f:
                json.dump({'updated_at': datetime.now().isoformat(), 'seen_urls': list(self.seen_deals)[-2000:]}, f)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")

    def _rotate_ip(self):
        """–†–æ—Ç–∞—Ü–∏—è IP —á–µ—Ä–µ–∑ CHANGE_IP_URL"""
        if CHANGE_IP_URL:
            try:
                logger.info("üîÑ –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–º–µ–Ω—É IP (API —Ä–æ—Ç–∞—Ü–∏–∏)...")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å–º–µ–Ω—ã IP
                import requests as basic_requests
                resp = basic_requests.get(CHANGE_IP_URL, timeout=15)
                logger.info(f"‚úÖ –û—Ç–≤–µ—Ç API —Ä–æ—Ç–∞—Ü–∏–∏: {resp.status_code}")
                # –ñ–¥–µ–º –ø–æ–∫–∞ –ø—Ä–æ–∫—Å–∏ "–ø–æ–¥–Ω–∏–º–µ—Ç—Å—è" —Å –Ω–æ–≤—ã–º IP
                time.sleep(15) 
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–º–µ–Ω–µ IP: {e}")
                time.sleep(20)
        else:
            logger.info("‚è≥ CHANGE_IP_URL –Ω–µ –∑–∞–¥–∞–Ω, –ø—Ä–æ—Å—Ç–æ –∂–¥–µ–º...")
            time.sleep(30)

    def get_page(self, url: str) -> Optional[str]:
        proxies = {"http": PROXY_URL, "https": PROXY_URL} if PROXY_URL else None
        
        for attempt in range(MAX_RETRIES):
            try:
                if HAS_CFFI:
                    resp = self.session.get(url, impersonate=IMPERSONATE, proxies=proxies, timeout=TIMEOUT)
                else:
                    headers = {'User-Agent': 'Mozilla/5.0 Chrome/120.0.0.0'}
                    resp = self.session.get(url, headers=headers, proxies=proxies, timeout=TIMEOUT)

                if resp.status_code == 200:
                    if "firewall" in resp.text.lower() or "captcha" in resp.text.lower():
                        logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞. –†–æ—Ç–∏—Ä—É–µ–º IP...")
                        self._rotate_ip()
                        continue
                    return resp.text
                elif resp.status_code in [403, 429]:
                    logger.warning(f"‚ö†Ô∏è –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ {resp.status_code}. –†–æ—Ç–∏—Ä—É–µ–º IP...")
                    self._rotate_ip()
                    continue
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
                self._rotate_ip()
        return None

    def parse_listings(self, html: str) -> List[dict]:
        soup = BeautifulSoup(html, 'lxml')
        items = []
        blocks = soup.find_all('div', attrs={'data-marker': 'item'})
        
        for block in blocks:
            try:
                link_tag = block.find('a', attrs={'data-marker': 'item-title'})
                price_meta = block.find('meta', attrs={'itemprop': 'price'})
                if not link_tag or not price_meta: continue
                
                items.append({
                    'url': urljoin("https://www.avito.ru", link_tag.get('href', '')),
                    'title': link_tag.get('title', '').strip(),
                    'price': int(price_meta.get('content', 0))
                })
            except: continue
        return items

    def extract_model(self, title: str) -> Optional[str]:
        t = title.lower()
        patterns = [
            (r'macbook\s*pro\s*16.*m3', 'MacBook Pro 16 (2023, M3 Pro/Max)'),
            (r'macbook\s*pro\s*14.*m3', 'MacBook Pro 14 (2023, M3)'),
            (r'macbook\s*air\s*13.*m2', 'MacBook Air 13 (2022, M2)'),
            (r'macbook\s*air.*m1', 'MacBook Air 13 (2020, M1)'),
        ]
        for pattern, model in patterns:
            if re.search(pattern, t): return model
        return None

    def find_deals(self, listings: List[dict]) -> List[HotDeal]:
        deals = []
        for item in listings:
            if item['url'] in self.seen_deals: continue
            model = self.extract_model(item['title'])
            if not model: continue
            
            median = self.prices_db.get(model)
            if not median or item['price'] < (median * 0.4): continue
                
            if item['price'] <= (median * HOT_DEAL_THRESHOLD):
                discount = (1 - item['price'] / median) * 100
                deals.append(HotDeal(
                    url=item['url'], title=item['title'], price=item['price'],
                    median_price=median, discount_percent=round(discount, 1),
                    model=model, found_at=datetime.now().isoformat()
                ))
        return deals

    def send_notifications(self, deals: List[HotDeal]):
        if not TELEGRAM_URL or not deals: return
        for deal in deals:
            try:
                text = (f"üî• <b>{deal.model}</b>\nüí∞ –¶–µ–Ω–∞: <b>{deal.price:,} ‚ÇΩ</b>\n"
                        f"üìâ –ú–µ–¥–∏–∞–Ω–∞: {deal.median_price:,} ‚ÇΩ (-{deal.discount_percent}%)\n"
                        f"üîó <a href='{deal.url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ –ê–≤–∏—Ç–æ</a>")
                cffi_requests.post(TELEGRAM_URL, json={"text": text, "parse_mode": "HTML"}, timeout=10)
            except Exception as e: logger.error(f"–û—à–∏–±–∫–∞ TG: {e}")

    def run(self):
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...")
        html = self.get_page(SCAN_URL)
        if html:
            listings = self.parse_listings(html)
            hot_deals = self.find_deals(listings)
            if hot_deals:
                self.send_notifications(hot_deals)
                for d in hot_deals: self.seen_deals.add(d.url)
                self._save_seen()
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {len(hot_deals)}")
            else:
                logger.info("ü§∑ –í—ã–≥–æ–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–µ—Ç")
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")

if __name__ == "__main__":
    AvitoScanner().run()
