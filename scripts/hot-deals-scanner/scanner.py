#!/usr/bin/env python3
import json
import os
import re
import time
import random
import logging
import urllib3
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: pip install requests beautifulsoup4 lxml")
    exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("Scanner")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_FILE = Path("public/data/seen-hot-deals.json")
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
SCAN_URL = os.environ.get('SCAN_URL')
PROXY_URL = os.environ.get('PROXY_URL', '').strip().strip('"').strip("'")
CHANGE_IP_URL = os.environ.get('CHANGE_IP_URL', '').strip().strip('"').strip("'")

# –ü–æ—Ä–æ–≥ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 10% –æ—Ç –Ω–∏–∑–∞ —Ä—ã–Ω–∫–∞
PRICE_THRESHOLD_FACTOR = 1.10 

# –°–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ "–≥–æ—Ä—è—á–∏—Ö" –ø—Ä–æ–¥–∞–≤—Ü–æ–≤
URGENT_KEYWORDS = ['—Å—Ä–æ—á–Ω–æ', '—Ç–æ—Ä–≥', '—É—Å—Ç—É–ø–ª—é', '–ø–µ—Ä–µ–µ–∑–¥', '—Å–µ–≥–æ–¥–Ω—è', '–±—ã—Å—Ç—Ä–æ', '–¥–∏—Å–∫–æ–Ω—Ç', '–≤–æ–∑–º–æ–∂–µ–Ω —Ç–æ—Ä–≥', '–æ—Ç–¥–∞–º –∑–∞']

# –°–ª–æ–≤–∞ –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –æ—Ç—Å–µ–≤–∞ –º—É—Å–æ—Ä–∞ –∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–∞–∫–æ–≤
BAD_KEYWORDS = [
    'mdm', '–∑–∞–ª–æ—á–µ–Ω', '—Ä–∞–∑–±–∏—Ç–∞', '—Ä–∞–∑–±–∏—Ç', '—Ä–µ–º–æ–Ω—Ç', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', 'icloud', 
    '–∑–∞–ø—á–∞—Å—Ç', '—ç–∫—Ä–∞–Ω', '–º–∞—Ç—Ä–∏—Ü', '–¥–µ—Ñ–µ–∫—Ç', '–∞–∫–∫–∞—É–Ω—Ç', '–∫–æ—Ä–æ–±–∫–∞', '—á–µ—Ö–æ–ª',
    '–ø–æ–¥ –∑–∞–∫–∞–∑', '—Å—Ä–æ–∫ –¥–æ—Å—Ç–∞–≤–∫–∏', '–ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞', '–∑–∞–º–µ–Ω–∞', '–º–µ–Ω—è–ª–∏', '–º–µ–Ω—è–ª',
    '–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω', '—Ä–µ—Ñ', 'refurbished', '–∑–∞–ª–∏—Ç', '–≥–ª—é—á–∏—Ç', '–ø–æ–ª–æ—Å—ã', '–ø—è—Ç–Ω–∞',
    '–≤ —Ä–∞–∑–±–æ—Ä', '–Ω–∞ —á–∞—Å—Ç–∏', '–ø–∞—Ä–æ–ª—å', '–æ–±—Ö–æ–¥'
]

def clean_url(url: str) -> str:
    """–û—á–∏—â–∞–µ—Ç URL –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
    return url.split('?')[0]

def extract_specs(text: str):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç RAM –∏ SSD. 
    –õ–æ–≥–∏–∫–∞: –ø–µ—Ä–≤–æ–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ - RAM, –≤—Ç–æ—Ä–æ–µ - SSD.
    """
    text = text.lower().replace(' ', '')
    matches = re.findall(r'(\d+)(?:gb|–≥–±|tb|—Ç–±)', text)
    
    ram, ssd = 8, 256
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≥–æ–¥–∞ 2018-2026
    clean_matches = [m for m in matches if not (2018 <= int(m) <= 2026)]
    
    if len(clean_matches) >= 2:
        ram = int(clean_matches[0])
        ssd_val = int(clean_matches[1])
        # –ï—Å–ª–∏ –≤—Ç–æ—Ä–æ–µ —á–∏—Å–ª–æ –º–∞–ª–µ–Ω—å–∫–æ–µ (1, 2, 4) - —ç—Ç–æ –¢–µ—Ä–∞–±–∞–π—Ç—ã
        ssd = ssd_val * 1024 if ssd_val <= 8 else ssd_val
    elif len(clean_matches) == 1:
        val = int(clean_matches[0])
        # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ —á–∏—Å–ª–æ, –≥–∞–¥–∞–µ–º RAM —ç—Ç–æ –∏–ª–∏ SSD –ø–æ –≤–µ–ª–∏—á–∏–Ω–µ
        if val in [8, 16, 18, 24, 32, 36, 48, 64, 96, 128]: ram = val
        else: ssd = val
            
    return ram, ssd

class AvitoScanner:
    def __init__(self):
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–∫—Å–∏
        p_str = PROXY_URL
        if p_str and not p_str.startswith('http'):
            p_str = f"http://{p_str}"
        self.proxies = {"http": p_str, "https": p_str} if p_str else None
        
        self.prices = {}
        if PRICES_FILE.exists():
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for s in data.get('stats', []):
                    # –ö–ª—é—á: (–º–æ–¥–µ–ª—å_–ª–æ–≤–µ—Ä, ram, ssd)
                    self.prices[(s['model_name'].lower(), int(s['ram']), int(s['ssd']))] = s
            logger.info(f"üìä –ë–∞–∑–∞ —Ü–µ–Ω –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(self.prices)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π")

        self.seen = set()
        if SEEN_FILE.exists():
            try:
                with open(SEEN_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.seen = set(clean_url(u) for u in data.get('seen_urls', []))
                logger.info(f"üëÅ –ò—Å—Ç–æ—Ä–∏—è: {len(self.seen)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            except: pass

    def rotate_ip(self):
        if CHANGE_IP_URL:
            try:
                requests.get(CHANGE_IP_URL, timeout=15, verify=False)
                time.sleep(12)
            except: pass

    def get_with_retry(self, url):
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        for attempt in range(3):
            try:
                resp = requests.get(url, headers=headers, proxies=self.proxies, timeout=30, verify=False)
                if resp.status_code == 200: return resp
                if resp.status_code in [403, 429]: self.rotate_ip()
            except:
                self.rotate_ip()
                time.sleep(5)
        return None

    def deep_analyze(self, url: str):
        """–ó–∞—Ö–æ–¥–∏—Ç –≤ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: –∏—â–µ—Ç –ê–ö–ë –∏ –°—Ä–æ—á–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ"""
        resp = self.get_with_retry(url)
        if not resp: return None, False
        try:
            soup = BeautifulSoup(resp.text, 'lxml')
            desc = soup.find('div', attrs={'data-marker': 'item-description'})
            text = desc.get_text().lower() if desc else ""
            
            # –ü–æ–∏—Å–∫ —Ü–∏–∫–ª–æ–≤
            cycles = None
            c_match = re.search(r'(\d+)\s*(?:—Ü–∏–∫–ª|cycle|—Ü\.|cyc)', text)
            if c_match: cycles = int(c_match.group(1))
            
            # –ü–æ–∏—Å–∫ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏
            is_urgent = any(word in text for word in URGENT_KEYWORDS)
            
            return cycles, is_urgent
        except: return None, False

    def notify(self, title, price, market_low, buyout, ram, ssd, url, cycles, is_urgent, is_avito_low):
        if not TELEGRAM_URL: return
        
        badges = []
        if is_urgent: badges.append("üö® <b>–°–†–û–ß–ù–û / –¢–û–†–ì</b>")
        if is_avito_low: badges.append("üìâ <b>–ù–ò–ñ–ï –†–´–ù–ö–ê (–ê–í–ò–¢–û)</b>")
        if cycles and cycles < 150: badges.append("üîã <b>–ê–ö–ë –ò–î–ï–ê–õ</b>")
        
        status_line = " | ".join(badges) if badges else "üéØ <b>–ù–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç!</b>"
        
        text = (
            f"{status_line}\n\n"
            f"üíª {title}\n"
            f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥: <b>{ram}GB / {ssd}GB</b>\n"
            f"üí∞ –¶–µ–Ω–∞ —Å–µ–π—á–∞—Å: <b>{price:,} ‚ÇΩ</b>\n"
            f"üìâ –ù–∏–∑ —Ä—ã–Ω–∫–∞: {market_low:,} ‚ÇΩ\n"
            f"ü§ù –¢–≤–æ–π –≤—ã–∫—É–ø: {buyout:,} ‚ÇΩ\n"
            f"‚ö° –¶–∏–∫–ª—ã: {cycles if cycles else '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}\n"
            f"üîó <a href='{url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ Avito</a>"
        )
        try:
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–∞–ø—Ä—è–º—É—é –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
            requests.post(TELEGRAM_URL, json={"text": text, "parse_mode": "HTML"}, timeout=10, proxies=None)
            logger.info(f"‚úÖ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {price} —Ä—É–±.")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Telegram: {e}")

    def run(self):
        if not SCAN_URL: return
        logger.info("üé¨ –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–æ–≤–∏–Ω–æ–∫...")
        
        resp = self.get_with_retry(SCAN_URL)
        if not resp:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å SCAN_URL")
            return

        soup = BeautifulSoup(resp.text, 'lxml')
        items = soup.select('[data-marker="item"]')
        logger.info(f"üîé –ù–∞–π–¥–µ–Ω–æ {len(items)} –Ω–æ–≤–∏–Ω–æ–∫.")
        
        found_matches = 0
        for item in items:
            try:
                link_tag = item.select_one('[data-marker="item-title"]')
                raw_url = urljoin("https://www.avito.ru", link_tag['href'])
                url = clean_url(raw_url)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏
                if url in self.seen: continue
                
                # 1. –ß–∏—Ç–∞–µ–º –ø—Ä–µ–≤—å—é –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º—É—Å–æ—Ä–∞
                raw_title = link_tag.get('title', '')
                snippet_tag = item.select_one('[data-marker="item-description"]')
                snippet_text = snippet_tag.get_text().lower() if snippet_tag else ""
                
                full_preview_text = (raw_title + " " + snippet_text).lower()
                
                # –ú–≥–Ω–æ–≤–µ–Ω–Ω—ã–π –æ—Ç—Å–µ–≤
                if any(word in full_preview_text for word in BAD_KEYWORDS):
                    self.seen.add(url)
                    continue

                price_tag = item.select_one('[itemprop="price"]')
                price = int(price_tag['content']) if price_tag else 0
                if price < 15000: continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–π–¥–∂–∞ –ê–≤–∏—Ç–æ
                is_avito_low = any(x in item.get_text().lower() for x in ["–Ω–∏–∂–µ —Ä—ã–Ω–æ—á–Ω–æ–π", "—Ü–µ–Ω–∞ –Ω–∏–∂–µ", "—Ö–æ—Ä–æ—à–∞—è —Ü–µ–Ω–∞"])

                # –ò–∑–≤–ª–µ–∫–∞–µ–º RAM/SSD
                ram, ssd = extract_specs(full_preview_text)
                
                # –ò—â–µ–º –≤ –±–∞–∑–µ
                matched_stat = None
                for (m_name, m_ram, m_ssd), stat in self.prices.items():
                    keywords = re.findall(r'[a-z0-9]+', m_name)
                    if all(word in raw_title.lower() for word in keywords) and m_ram == ram and m_ssd == ssd:
                        matched_stat = stat
                        break
                
                if matched_stat:
                    market_low = matched_stat['min_price']
                    
                    # –£–°–õ–û–í–ò–Ø –û–¢–ü–†–ê–í–ö–ò: 
                    # –¶–µ–Ω–∞ –æ–∫ –ò–õ–ò –±–µ–π–¥–∂ –ê–≤–∏—Ç–æ –ò–õ–ò —Å—Ä–æ—á–Ω–æ—Å—Ç—å
                    should_notify = False
                    if price <= int(market_low * PRICE_THRESHOLD_FACTOR): should_notify = True
                    if is_avito_low: should_notify = True
                    
                    # –ï—Å–ª–∏ –µ—â–µ –Ω–µ —Ä–µ—à–∏–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å, –∑–∞–≥–ª—è–Ω–µ–º –≤ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç '—Å—Ä–æ—á–Ω–æ/—Ç–æ—Ä–≥'
                    cycles, is_urgent = None, False
                    if not should_notify:
                        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Å–æ–≤–ø–∞–ª–∞, –Ω–æ —Ü–µ–Ω–∞ —á—É—Ç—å –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
                        cycles, is_urgent = self.deep_analyze(raw_url)
                        if is_urgent: should_notify = True
                    else:
                        # –ï—Å–ª–∏ —É–∂–µ —Ä–µ—à–∏–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å, –ø—Ä–æ—Å—Ç–æ –∑–∞–±–µ—Ä–µ–º —Ü–∏–∫–ª—ã
                        cycles, is_urgent = self.deep_analyze(raw_url)

                    if should_notify:
                        self.notify(raw_title, price, market_low, matched_stat['buyout_price'], ram, ssd, url, cycles, is_urgent, is_avito_low)
                        self.seen.add(url)
                        found_matches += 1
                        time.sleep(random.uniform(2, 4))
            except: continue

        if found_matches > 0:
            with open(SEEN_FILE, 'w', encoding='utf-8') as f:
                json.dump({"updated_at": datetime.now().isoformat(), "seen_urls": list(self.seen)[-4500:]}, f)
        
        logger.info(f"üèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {found_matches}")

if __name__ == "__main__":
    AvitoScanner().run()
