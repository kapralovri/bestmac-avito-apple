#!/usr/bin/env python3
import json
import os
import re
import time
import random
import logging
import urllib3
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List, Set, Dict
from pathlib import Path
from urllib.parse import urljoin

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: pip install requests beautifulsoup4 lxml")
    exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("Scanner")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_FILE = Path("public/data/seen-hot-deals.json")
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
SCAN_URL = os.environ.get('SCAN_URL')
PROXY_URL = os.environ.get('PROXY_URL')
CHANGE_IP_URL = os.environ.get('CHANGE_IP_URL')

BAD_KEYWORDS = ['icloud', '–∑–∞–ø—á–∞—Å—Ç–∏', '–±–∏—Ç—ã–π', '—Ä–∞–∑–±–∏—Ç', '–±–ª–æ–∫–∏—Ä–æ–≤', '—ç–∫—Ä–∞–Ω', '–¥–µ—Ñ–µ–∫—Ç', 'mdm', '–∞–∫–∫–∞—É–Ω—Ç']
URGENT_KEYWORDS = ['—Å—Ä–æ—á–Ω–æ', '—Ç–æ—Ä–≥', '—Å–µ–≥–æ–¥–Ω—è', '–ø–µ—Ä–µ–µ–∑–¥', '–æ—Ç–¥–∞—é', '–¥–µ—à–µ–≤–æ', '–±—ã—Å—Ç—Ä–æ']

def clean_url(url: str) -> str:
    return url.split('?')[0]

def extract_specs(text: str):
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –∫–æ–Ω—Ñ–∏–≥–∞. 
    –û–±—ã—á–Ω–æ –Ω–∞ –ê–≤–∏—Ç–æ –ø–∏—à—É—Ç '–ú–æ–¥–µ–ª—å RAM/SSD' –∏–ª–∏ '–ú–æ–¥–µ–ª—å RAM SSD'.
    """
    text = text.lower().replace(' ', '')
    # –ò—â–µ–º –≤—Å–µ —á–∏—Å–ª–∞, –∑–∞ –∫–æ—Ç–æ—Ä—ã–º–∏ –∏–¥–µ—Ç gb, –≥–±, tb –∏–ª–∏ —Ç–±
    matches = re.findall(r'(\d+)(?:gb|–≥–±|tb|—Ç–±)', text)
    
    ram = 8
    ssd = 256
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –∏–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≥–æ–¥–∞ (2020-2025)
    clean_matches = [m for m in matches if not (2018 <= int(m) <= 2026)]
    
    if len(clean_matches) >= 2:
        # –ü–µ—Ä–≤–æ–µ —á–∏—Å–ª–æ - RAM, –≤—Ç–æ—Ä–æ–µ - SSD
        ram = int(clean_matches[0])
        ssd_val = int(clean_matches[1])
        # –ï—Å–ª–∏ –≤—Ç–æ—Ä–æ–µ —á–∏—Å–ª–æ –º–∞–ª–µ–Ω—å–∫–æ–µ (1, 2, 4, 8) - —ç—Ç–æ –¢–µ—Ä–∞–±–∞–π—Ç—ã
        ssd = ssd_val * 1024 if ssd_val <= 8 else ssd_val
    elif len(clean_matches) == 1:
        # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ —á–∏—Å–ª–æ, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–Ω—è—Ç—å RAM —ç—Ç–æ –∏–ª–∏ SSD
        val = int(clean_matches[0])
        if val in [8, 16, 18, 24, 32, 36, 48, 64]:
            ram = val
        else:
            ssd = val
            
    return ram, ssd

class AvitoScanner:
    def __init__(self):
        raw_p = os.environ.get("PROXY_URL", "").strip().strip('"').strip("'")
        self.proxies = {"http": f"http://{raw_p}", "https": f"http://{raw_p}"} if raw_p else None
        
        self.prices = {}
        if PRICES_FILE.exists():
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for s in data.get('stats', []):
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É: –∫–ª—é—á (–º–æ–¥–µ–ª—å_–ª–æ–≤–µ—Ä, ram, ssd)
                    self.prices[(s['model_name'].lower(), int(s['ram']), int(s['ssd']))] = s
        
        self.seen = set()
        if SEEN_FILE.exists():
            try:
                with open(SEEN_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.seen = set(clean_url(u) for u in data.get('seen_urls', []))
            except: pass

    def deep_analyze(self, url: str):
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"}
            resp = requests.get(url, headers=headers, proxies=self.proxies, timeout=15, verify=False)
            if resp.status_code != 200: return None, False
            soup = BeautifulSoup(resp.text, 'lxml')
            desc = soup.find('div', attrs={'data-marker': 'item-description'})
            text = desc.get_text().lower() if desc else ""
            cycles = None
            c_match = re.search(r'(\d+)\s*(?:—Ü–∏–∫–ª|cycle|—Ü\.|cyc)', text)
            if c_match: cycles = int(c_match.group(1))
            urgent = any(word in text for word in URGENT_KEYWORDS)
            return cycles, urgent
        except: return None, False

    def run(self):
        if not SCAN_URL: return
        logger.info("üé¨ –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...")
        
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        try:
            resp = requests.get(SCAN_URL, headers=headers, proxies=self.proxies, timeout=30, verify=False)
            if resp.status_code != 200: return
            soup = BeautifulSoup(resp.text, 'lxml')
            items = soup.select('[data-marker="item"]')
            
            for item in items:
                try:
                    link_tag = item.select_one('[data-marker="item-title"]')
                    raw_url = urljoin("https://www.avito.ru", link_tag['href'])
                    url = clean_url(raw_url)
                    
                    if url in self.seen: continue
                    
                    raw_title = link_tag.get('title', '')
                    title_low = raw_title.lower()
                    if any(word in title_low for word in BAD_KEYWORDS): continue

                    price = int(item.select_one('[itemprop="price"]')['content'])
                    
                    # –ü–†–ê–í–ò–õ–¨–ù–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–û–ù–§–ò–ì–ê
                    ram, ssd = extract_specs(title_low)
                    
                    # –ü–û–ò–°–ö –í –ë–ê–ó–ï
                    matched_stat = None
                    for (m_name, m_ram, m_ssd), stat in self.prices.items():
                        keywords = re.findall(r'[a-z0-9]+', m_name)
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å + –°–¢–†–û–ì–û RAM + –°–¢–†–û–ì–û SSD
                        if all(word in title_low for word in keywords) and m_ram == ram and m_ssd == ssd:
                            matched_stat = stat
                            break
                    
                    if matched_stat:
                        market_low = matched_stat['min_price'] # –ò—Å–ø–æ–ª—å–∑—É–µ–º min_price (–ù–∏–∑ —Ä—ã–Ω–∫–∞)
                        if price <= market_low * 1.02: # –£–≤–µ–¥–æ–º–ª—è–µ–º, –µ—Å–ª–∏ —Ü–µ–Ω–∞ –æ–∫–æ–ª–æ –∏–ª–∏ –Ω–∏–∂–µ "–ù–∏–∑–∞"
                            cycles, urgent_desc = self.deep_analyze(raw_url)
                            
                            text = (
                                f"üéØ <b>–ù–∞—à–µ–ª –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ –ù–ò–ó–£ —Ä—ã–Ω–∫–∞!</b>\n\n"
                                f"üíª {raw_title}\n"
                                f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥: <b>{ram}GB / {ssd}GB</b>\n"
                                f"üí∞ –¶–µ–Ω–∞ —Å–µ–π—á–∞—Å: <b>{price:,} ‚ÇΩ</b>\n"
                                f"üìâ –ù–∏–∑ —Ä—ã–Ω–∫–∞: {market_low:,} ‚ÇΩ\n"
                                f"ü§ù –¢–≤–æ–π –≤—ã–∫—É–ø: {matched_stat['buyout_price']:,} ‚ÇΩ\n"
                                f"‚ö° –¶–∏–∫–ª—ã: {cycles if cycles else '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}\n"
                                f"üîó <a href='{url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ Avito</a>"
                            )
                            requests.post(TELEGRAM_URL, json={"text": text, "parse_mode": "HTML"}, timeout=10)
                            self.seen.add(url)
                            time.sleep(2)
                except: continue

            with open(SEEN_FILE, 'w', encoding='utf-8') as f:
                json.dump({"updated_at": datetime.now().isoformat(), "seen_urls": list(self.seen)[-4000:]}, f)

        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    AvitoScanner().run()
