#!/usr/bin/env python3
import json
import os
import re
import time
import random
import logging
import urllib3
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: pip install requests beautifulsoup4 lxml")
    exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("Scanner")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_FILE = Path("public/data/seen-hot-deals.json")
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
SCAN_URL = os.environ.get('SCAN_URL')
PROXY_URL = os.environ.get('PROXY_URL', '').strip().strip('"').strip("'")
CHANGE_IP_URL = os.environ.get('CHANGE_IP_URL', '').strip().strip('"').strip("'")

# –ü–æ—Ä–æ–≥: 10% –æ—Ç –Ω–∏–∑–∞ —Ä—ã–Ω–∫–∞
PRICE_THRESHOLD_FACTOR = 1.10 
URGENT_KEYWORDS = ['—Å—Ä–æ—á–Ω–æ', '—Ç–æ—Ä–≥', '—É—Å—Ç—É–ø–ª—é', '–ø–µ—Ä–µ–µ–∑–¥', '—Å–µ–≥–æ–¥–Ω—è', '–±—ã—Å—Ç—Ä–æ', '–¥–∏—Å–∫–æ–Ω—Ç', '–≤–æ–∑–º–æ–∂–µ–Ω —Ç–æ—Ä–≥', '–æ—Ç–¥–∞–º –∑–∞']
BAD_KEYWORDS = ['–ø–æ–¥ –∑–∞–∫–∞–∑', '—Å—Ä–æ–∫ –¥–æ—Å—Ç–∞–≤–∫–∏', '–ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞', '–∏–∑ –µ–≤—Ä–æ–ø—ã', '–∏–∑ —Å—à–∞', 'icloud', '–∑–∞–ø—á–∞—Å—Ç–∏', '–±–∏—Ç—ã–π', '—Ä–∞–∑–±–∏—Ç', '–±–ª–æ–∫–∏—Ä–æ–≤', '—ç–∫—Ä–∞–Ω', '–¥–µ—Ñ–µ–∫—Ç', 'mdm', '–∞–∫–∫–∞—É–Ω—Ç']

def clean_url(url: str) -> str:
    return url.split('?')[0]

def extract_specs(text: str):
    text = text.lower().replace(' ', '')
    matches = re.findall(r'(\d+)(?:gb|–≥–±|tb|—Ç–±)', text)
    ram, ssd = 8, 256
    clean_matches = [m for m in matches if not (2018 <= int(m) <= 2026)]
    if len(clean_matches) >= 2:
        ram = int(clean_matches[0])
        ssd_val = int(clean_matches[1])
        ssd = ssd_val * 1024 if ssd_val <= 8 else ssd_val
    elif len(clean_matches) == 1:
        val = int(clean_matches[0])
        if val in [8, 16, 18, 24, 32, 36, 48, 64]: ram = val
        else: ssd = val
    return ram, ssd

class AvitoScanner:
    def __init__(self):
        logger.info("üõ† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
        p_str = PROXY_URL
        if p_str and not p_str.startswith('http'):
            p_str = f"http://{p_str}"
        self.proxies = {"http": p_str, "https": p_str} if p_str else None
        
        self.prices = {}
        if PRICES_FILE.exists():
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                stats = data.get('stats', [])
                for s in stats:
                    self.prices[(s['model_name'].lower(), int(s['ram']), int(s['ssd']))] = s
                logger.info(f"üìä –ë–∞–∑–∞ —Ü–µ–Ω: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.prices)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π.")
        else:
            logger.error("‚ùå –§–∞–π–ª avito-prices.json –ù–ï –ù–ê–ô–î–ï–ù!")

        self.seen = set()
        if SEEN_FILE.exists():
            try:
                with open(SEEN_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.seen = set(clean_url(u) for u in data.get('seen_urls', []))
                logger.info(f"üëÅ –ò—Å—Ç–æ—Ä–∏—è: {len(self.seen)} –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫.")
            except: pass

    def rotate_ip(self):
        if CHANGE_IP_URL:
            try:
                logger.info("üîÑ –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–º–µ–Ω—É IP (aproxy.site)...")
                requests.get(CHANGE_IP_URL, timeout=15, verify=False)
                time.sleep(15)
                return True
            except Exception as e:
                logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–º–µ–Ω—ã IP: {e}")
        return False

    def get_with_retry(self, url):
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        for attempt in range(3):
            try:
                logger.info(f"üì° –ü–æ–ø—ã—Ç–∫–∞ {attempt+1}/3 –∑–∞–ø—Ä–æ—Å–∞ –∫ –ê–≤–∏—Ç–æ...")
                resp = requests.get(url, headers=headers, proxies=self.proxies, timeout=30, verify=False)
                if resp.status_code == 200:
                    logger.info("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω–∞.")
                    return resp
                logger.warning(f"‚ö†Ô∏è –°—Ç–∞—Ç—É—Å-–∫–æ–¥: {resp.status_code}")
                if resp.status_code in [403, 429]:
                    self.rotate_ip()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")
                self.rotate_ip()
                time.sleep(5)
        return None

    def deep_analyze(self, url: str):
        resp = self.get_with_retry(url)
        if not resp: return None, False
        try:
            soup = BeautifulSoup(resp.text, 'lxml')
            desc = soup.find('div', attrs={'data-marker': 'item-description'})
            text = desc.get_text().lower() if desc else ""
            cycles = None
            c_match = re.search(r'(\d+)\s*(?:—Ü–∏–∫–ª|cycle|—Ü\.|cyc)', text)
            if c_match: cycles = int(c_match.group(1))
            is_urgent = any(word in text for word in URGENT_KEYWORDS)
            return cycles, is_urgent
        except: return None, False

    def notify(self, title, price, market_low, buyout, ram, ssd, url, cycles, is_urgent, is_avito_low):
        if not TELEGRAM_URL: return
        badges = []
        if is_urgent: badges.append("üö® <b>–°–†–û–ß–ù–û / –¢–û–†–ì</b>")
        if is_avito_low: badges.append("üìâ <b>–ù–ò–ñ–ï –†–´–ù–ö–ê (–ê–í–ò–¢–û)</b>")
        if cycles and cycles < 150: badges.append("üîã <b>–ê–ö–ë –ò–î–ï–ê–õ</b>")
        status_line = " | ".join(badges) if badges else "üéØ <b>–ù–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç!</b>"
        
        text = (
            f"{status_line}\n\n"
            f"üíª {title}\n"
            f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥: <b>{ram}GB / {ssd}GB</b>\n"
            f"üí∞ –¶–µ–Ω–∞ —Å–µ–π—á–∞—Å: <b>{price:,} ‚ÇΩ</b>\n"
            f"üìâ –ù–∏–∑ —Ä—ã–Ω–∫–∞: {market_low:,} ‚ÇΩ\n"
            f"ü§ù –¢–≤–æ–π –≤—ã–∫—É–ø: {buyout:,} ‚ÇΩ\n"
            f"‚ö° –¶–∏–∫–ª—ã: {cycles if cycles else '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}\n"
            f"üîó <a href='{url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ Avito</a>"
        )
        try:
            requests.post(TELEGRAM_URL, json={"text": text, "parse_mode": "HTML"}, timeout=10, proxies=None)
            logger.info(f"üì© –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram!")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Telegram: {e}")

    def run(self):
        if not SCAN_URL:
            logger.error("‚ùå SCAN_URL –Ω–µ –∑–∞–¥–∞–Ω!")
            return
        
        logger.info("üé¨ –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...")
        resp = self.get_with_retry(SCAN_URL)
        if not resp:
            logger.error("‚ùå –°–∫—Ä–∏–ø—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É (–±–∞–Ω –∏–ª–∏ –æ—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏).")
            return

        soup = BeautifulSoup(resp.text, 'lxml')
        items = soup.select('[data-marker="item"]')
        total_items = len(items)
        logger.info(f"üîé –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–∞–π–¥–µ–Ω–æ {total_items} –æ–±—ä—è–≤–ª–µ–Ω–∏–π.")
        
        model_matches = 0
        matches_found = 0
        new_history_count = 0

        for item in items:
            try:
                link_tag = item.select_one('[data-marker="item-title"]')
                raw_url = urljoin("https://www.avito.ru", link_tag['href'])
                url = clean_url(raw_url)
                if url in self.seen: continue

                price_tag = item.select_one('[itemprop="price"]')
                price = int(price_tag['content']) if price_tag else 0
                if price < 15000: continue

                raw_title = link_tag.get('title', '')
                title_low = raw_title.lower()
                if any(word in title_low for word in BAD_KEYWORDS): continue

                ram, ssd = extract_specs(title_low)
                
                matched_stat = None
                for (m_name, m_ram, m_ssd), stat in self.prices.items():
                    keywords = re.findall(r'[a-z0-9]+', m_name)
                    if all(word in title_low for word in keywords) and m_ram == ram and m_ssd == ssd:
                        matched_stat = stat
                        break
                
                if matched_stat:
                    model_matches += 1
                    market_low = matched_stat['min_price']
                    badge_text = item.get_text().lower()
                    is_avito_low = "–Ω–∏–∂–µ —Ä—ã–Ω–æ—á–Ω–æ–π" in badge_text or "—Ü–µ–Ω–∞ –Ω–∏–∂–µ" in badge_text
                    
                    # –£—Å–ª–æ–≤–∏–µ –æ—Ç–ø—Ä–∞–≤–∫–∏
                    if price <= int(market_low * PRICE_THRESHOLD_FACTOR) or is_avito_low:
                        logger.info(f"üî• MATCH: {raw_title} –∑–∞ {price} —Ä—É–±.")
                        cycles, is_urgent = self.deep_analyze(raw_url)
                        self.notify(raw_title, price, market_low, matched_stat['buyout_price'], ram, ssd, url, cycles, is_urgent, is_avito_low)
                        matches_found += 1
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–∞—à–∞
                    self.seen.add(url)
                    new_history_count += 1
            except Exception as e:
                continue

        logger.info(f"üèÅ –ò–¢–û–ì –†–ê–ë–û–¢–´:")
        logger.info(f"   - –í—Å–µ–≥–æ –Ω–æ–≤–∏–Ω–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {total_items}")
        logger.info(f"   - –ò–∑ –Ω–∏—Ö –Ω—É–∂–Ω—ã—Ö –Ω–∞–º –º–æ–¥–µ–ª–µ–π: {model_matches}")
        logger.info(f"   - –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π: {matches_found}")

        if matches_found == 0:
            logger.info("ü§∑ –ù–∏—á–µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –ø–æ —Ü–µ–Ω–µ –∏–ª–∏ –±–µ–π–¥–∂–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

        if new_history_count > 0:
            with open(SEEN_FILE, 'w', encoding='utf-8') as f:
                json.dump({"updated_at": datetime.now().isoformat(), "seen_urls": list(self.seen)[-4500:]}, f)
            logger.info(f"üíæ –ò—Å—Ç–æ—Ä–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞ (+{new_history_count} —Å—Å—ã–ª–æ–∫).")

if __name__ == "__main__":
    try:
        AvitoScanner().run()
    except Exception as e:
        logger.error(f"üí• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
