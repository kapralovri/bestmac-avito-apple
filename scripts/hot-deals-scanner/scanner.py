#!/usr/bin/env python3
import json
import os
import re
import time
import random
import logging
import urllib3
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: pip install requests beautifulsoup4 lxml")
    exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("Scanner")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_FILE = Path("public/data/seen-hot-deals.json")
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
SCAN_URL = os.environ.get('SCAN_URL')
PROXY_URL = os.environ.get('PROXY_URL', '').strip().strip('"').strip("'")
CHANGE_IP_URL = os.environ.get('CHANGE_IP_URL', '').strip().strip('"').strip("'")

# –£–°–õ–û–í–ò–Ø
PRICE_THRESHOLD_FACTOR = 1.10 
URGENT_KEYWORDS = ['—Å—Ä–æ—á–Ω–æ', '—Ç–æ—Ä–≥', '—É—Å—Ç—É–ø–ª—é', '–ø–µ—Ä–µ–µ–∑–¥', '—Å–µ–≥–æ–¥–Ω—è', '–±—ã—Å—Ç—Ä–æ', '–¥–∏—Å–∫–æ–Ω—Ç', '–≤–æ–∑–º–æ–∂–µ–Ω —Ç–æ—Ä–≥', '–æ—Ç–¥–∞–º –∑–∞']
BAD_KEYWORDS = ['icloud', '–∑–∞–ø—á–∞—Å—Ç–∏', '–±–∏—Ç—ã–π', '—Ä–∞–∑–±–∏—Ç', '–±–ª–æ–∫–∏—Ä–æ–≤', '—ç–∫—Ä–∞–Ω', '–¥–µ—Ñ–µ–∫—Ç', 'mdm', '–∞–∫–∫–∞—É–Ω—Ç']

def clean_url(url: str) -> str:
    return url.split('?')[0]

def extract_specs(text: str):
    text = text.lower().replace(' ', '')
    matches = re.findall(r'(\d+)(?:gb|–≥–±|tb|—Ç–±)', text)
    ram, ssd = 8, 256
    clean_matches = [m for m in matches if not (2018 <= int(m) <= 2026)]
    if len(clean_matches) >= 2:
        ram = int(clean_matches[0])
        ssd_val = int(clean_matches[1])
        ssd = ssd_val * 1024 if ssd_val <= 8 else ssd_val
    elif len(clean_matches) == 1:
        val = int(clean_matches[0])
        if val in [8, 16, 18, 24, 32, 36, 48, 64]: ram = val
        else: ssd = val
    return ram, ssd

class AvitoScanner:
    def __init__(self):
        p_str = PROXY_URL
        if p_str and not p_str.startswith('http'):
            p_str = f"http://{p_str}"
        self.proxies = {"http": p_str, "https": p_str} if p_str else None
        
        self.prices = {}
        if PRICES_FILE.exists():
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for s in data.get('stats', []):
                    self.prices[(s['model_name'].lower(), int(s['ram']), int(s['ssd']))] = s

        self.seen = set()
        if SEEN_FILE.exists():
            try:
                with open(SEEN_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.seen = set(clean_url(u) for u in data.get('seen_urls', []))
            except: pass

    def rotate_ip(self):
        if CHANGE_IP_URL:
            try:
                requests.get(CHANGE_IP_URL, timeout=15, verify=False)
                time.sleep(12)
            except: pass

    def get_with_retry(self, url):
        headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"}
        for attempt in range(2):
            try:
                resp = requests.get(url, headers=headers, proxies=self.proxies, timeout=25, verify=False)
                if resp.status_code == 200: return resp
                if resp.status_code in [403, 429]: self.rotate_ip()
            except: self.rotate_ip()
        return None

    def deep_analyze(self, url: str):
        """–ó–∞—Ö–æ–¥–∏—Ç –≤ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: –∏—â–µ—Ç –ê–ö–ë –∏ –°—Ä–æ—á–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ"""
        resp = self.get_with_retry(url)
        if not resp: return None, False
        try:
            soup = BeautifulSoup(resp.text, 'lxml')
            desc = soup.find('div', attrs={'data-marker': 'item-description'})
            text = desc.get_text().lower() if desc else ""
            
            # 1. –ò—â–µ–º —Ü–∏–∫–ª—ã
            cycles = None
            c_match = re.search(r'(\d+)\s*(?:—Ü–∏–∫–ª|cycle|—Ü\.|cyc)', text)
            if c_match: cycles = int(c_match.group(1))
            
            # 2. –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            is_urgent = any(word in text for word in URGENT_KEYWORDS)
            
            return cycles, is_urgent
        except: return None, False

    def notify(self, title, price, market_low, buyout, ram, ssd, url, cycles, is_urgent, is_avito_low):
        if not TELEGRAM_URL: return
        
        badges = []
        if is_urgent: badges.append("üö® <b>–°–†–û–ß–ù–û / –¢–û–†–ì</b>")
        if is_avito_low: badges.append("üìâ <b>–ù–ò–ñ–ï –†–´–ù–ö–ê (–ê–í–ò–¢–û)</b>")
        if cycles and cycles < 150: badges.append("üîã <b>–ê–ö–ë –ò–î–ï–ê–õ</b>")
        
        status_line = " | ".join(badges) if badges else "üéØ <b>–ù–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç!</b>"
        
        text = (
            f"{status_line}\n\n"
            f"üíª {title}\n"
            f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥: <b>{ram}GB / {ssd}GB</b>\n"
            f"üí∞ –¶–µ–Ω–∞ —Å–µ–π—á–∞—Å: <b>{price:,} ‚ÇΩ</b>\n"
            f"üìâ –ù–∏–∑ —Ä—ã–Ω–∫–∞: {market_low:,} ‚ÇΩ\n"
            f"ü§ù –¢–≤–æ–π –≤—ã–∫—É–ø: {buyout:,} ‚ÇΩ\n"
            f"‚ö° –¶–∏–∫–ª—ã: {cycles if cycles else '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}\n"
            f"üîó <a href='{url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ Avito</a>"
        )
        try:
            requests.post(TELEGRAM_URL, json={"text": text, "parse_mode": "HTML"}, timeout=10, proxies=None)
            logger.info(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {price} —Ä—É–±.")
        except: pass

    def run(self):
        if not SCAN_URL: return
        logger.info("üé¨ –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...")
        resp = self.get_with_retry(SCAN_URL)
        if not resp: return

        soup = BeautifulSoup(resp.text, 'lxml')
        items = soup.select('[data-marker="item"]')
        
        found_matches = 0
        for item in items:
            try:
                link_tag = item.select_one('[data-marker="item-title"]')
                raw_url = urljoin("https://www.avito.ru", link_tag['href'])
                url = clean_url(raw_url)
                if url in self.seen: continue

                price_tag = item.select_one('[itemprop="price"]')
                price = int(price_tag['content']) if price_tag else 0
                if price < 15000: continue

                raw_title = link_tag.get('title', '')
                title_low = raw_title.lower()
                if any(word in title_low for word in BAD_KEYWORDS): continue

                ram, ssd = extract_specs(title_low)
                
                matched_stat = None
                for (m_name, m_ram, m_ssd), stat in self.prices.items():
                    keywords = re.findall(r'[a-z0-9]+', m_name)
                    if all(word in title_low for word in keywords) and m_ram == ram and m_ssd == ssd:
                        matched_stat = stat
                        break
                
                if matched_stat:
                    market_low = matched_stat['min_price']
                    badge_text = item.get_text().lower()
                    is_avito_low = "–Ω–∏–∂–µ —Ä—ã–Ω–æ—á–Ω–æ–π" in badge_text or "—Ü–µ–Ω–∞ –Ω–∏–∂–µ" in badge_text
                    
                    # –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
                    cycles, is_urgent = self.deep_analyze(raw_url)
                    
                    # –£–°–õ–û–í–ò–Ø –û–¢–ü–†–ê–í–ö–ò: 
                    # 1. –¶–µ–Ω–∞ –Ω–∏–∑–∫–∞—è –ò–õ–ò 2. –ë–µ–π–¥–∂ –ê–≤–∏—Ç–æ –ò–õ–ò 3. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                    if price <= int(market_low * PRICE_THRESHOLD_FACTOR) or is_avito_low or is_urgent:
                        self.notify(raw_title, price, market_low, matched_stat['buyout_price'], ram, ssd, url, cycles, is_urgent, is_avito_low)
                        self.seen.add(url)
                        found_matches += 1
                        time.sleep(random.uniform(2, 4))
            except: continue

        if found_matches > 0:
            with open(SEEN_FILE, 'w', encoding='utf-8') as f:
                json.dump({"updated_at": datetime.now().isoformat(), "seen_urls": list(self.seen)[-4500:]}, f)

if __name__ == "__main__":
    AvitoScanner().run()
