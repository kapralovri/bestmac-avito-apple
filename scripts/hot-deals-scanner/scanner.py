#!/usr/bin/env python3
import json
import os
import re
import time
import random
import logging
import urllib3
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List, Set, Dict
from pathlib import Path
from urllib.parse import urljoin

# –û—Ç–∫–ª—é—á–∞–µ–º –≤–æ—Ä–Ω–∏–Ω–≥–∏ SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: Install requirements: pip install requests beautifulsoup4 lxml")
    exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("Scanner")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
SCAN_URL = os.environ.get('SCAN_URL')
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_FILE = Path("public/data/seen-hot-deals.json")

URGENT_KEYWORDS = ['—Å—Ä–æ—á–Ω–æ', '—Ç–æ—Ä–≥', '—Å–µ–≥–æ–¥–Ω—è', '–ø–µ—Ä–µ–µ–∑–¥', '–æ—Ç–¥–∞—é', '–¥–µ—à–µ–≤–æ', '–±—ã—Å—Ç—Ä–æ']
BAD_KEYWORDS = ['icloud', '–∑–∞–ø—á–∞—Å—Ç–∏', '–±–∏—Ç—ã–π', '—Ä–∞–∑–±–∏—Ç', '–±–ª–æ–∫–∏—Ä–æ–≤', '—ç–∫—Ä–∞–Ω', '–¥–µ—Ñ–µ–∫—Ç', 'mdm', '–∞–∫–∫–∞—É–Ω—Ç']

@dataclass
class HotDeal:
    url: str
    title: str
    price: int
    market_low: int
    buyout: int
    discount_percent: float
    model: str
    date: str
    ram: int
    ssd: int
    battery_cycles: Optional[int] = None
    is_urgent: bool = False

def extract_specs(text: str):
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ RAM –∏ SSD (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø—É—Ç–∞–Ω–∏—Ü—ã —Å –≥–æ–¥–æ–º)"""
    text = text.lower().replace(' ', '')
    
    # –ò—â–µ–º RAM
    ram = 8
    ram_match = re.search(r'\b(8|16|18|24|32|36|48|64|96|128)(?:gb|–≥–±)\b', text)
    if ram_match:
        ram = int(ram_match.group(1))
    
    # –ò—â–µ–º SSD (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≥–æ–¥—ã 2010-2025)
    ssd = 256
    ssd_matches = re.findall(r'(\d+)(?:gb|–≥–±|tb|—Ç–±)', text)
    for val_str in ssd_matches:
        val = int(val_str)
        if 2010 <= val <= 2026: continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–æ–¥–∞
        if val in [128, 256, 512, 1, 2, 4]: # –¢–∏–ø–∏—á–Ω—ã–µ –æ–±—ä–µ–º—ã
            if val <= 8: # –≠—Ç–æ —Ç–µ—Ä–∞–±–∞–π—Ç—ã
                ssd = val * 1024
            else:
                ssd = val
            break
    return ram, ssd

class AvitoScanner:
    def __init__(self):
        self.prices = {}
        if PRICES_FILE.exists():
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for s in data.get('stats', []):
                    # –ö–ª—é—á: (–ú–æ–¥–µ–ª—å_–ª–æ–≤–µ—Ä, RAM, SSD)
                    name = s['model_name'].lower()
                    self.prices[(name, s['ram'], s['ssd'])] = s
        
        self.seen = set()
        if SEEN_FILE.exists():
            try:
                with open(SEEN_FILE, 'r', encoding='utf-8') as f:
                    self.seen = set(json.load(f).get('seen_urls', []))
            except: pass

    def deep_analyze(self, url: str):
        """–ó–∞—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏–∫–ª–æ–≤ –∏ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏"""
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            resp = requests.get(url, headers=headers, timeout=20)
            if resp.status_code != 200: return None, False
            
            soup = BeautifulSoup(resp.text, 'lxml')
            desc = soup.find('div', attrs={'data-marker': 'item-description'})
            text = desc.get_text().lower() if desc else ""
            
            cycles = None
            c_match = re.search(r'(\d+)\s*(?:—Ü–∏–∫–ª|cycle|—Ü\.|cyc)', text)
            if c_match: cycles = int(c_match.group(1))
            
            urgent = any(word in text for word in URGENT_KEYWORDS)
            return cycles, urgent
        except:
            return None, False

    def notify(self, d: HotDeal):
        if not TELEGRAM_URL: return
        
        status = ""
        if d.is_urgent: status += "üö® <b>–°–†–û–ß–ù–û</b> "
        if d.battery_cycles and d.battery_cycles < 150: status += "üîã <b>–ê–ö–ë –ò–î–ï–ê–õ</b>"
        
        text = (
            f"üéØ <b>–ù–∞—à–µ–ª –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ –ù–ò–ó–£ —Ä—ã–Ω–∫–∞!</b>\n{status}\n\n"
            f"üíª {d.title}\n"
            f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥: <b>{d.ram}GB / {d.ssd}GB</b>\n"
            f"üí∞ –¶–µ–Ω–∞ —Å–µ–π—á–∞—Å: <b>{d.price:,} ‚ÇΩ</b>\n"
            f"üìâ –ù–∏–∑ —Ä—ã–Ω–∫–∞: {d.market_low:,} ‚ÇΩ\n"
            f"ü§ù –¢–≤–æ–π –≤—ã–∫—É–ø: {d.buyout:,} ‚ÇΩ\n"
            f"‚ö° –¶–∏–∫–ª—ã: {d.battery_cycles if d.battery_cycles else '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}\n"
            f"üïí {d.date}\n\n"
            f"üîó <a href='{d.url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ Avito</a>"
        )
        try:
            requests.post(TELEGRAM_URL, json={"text": text, "parse_mode": "HTML"}, timeout=10)
        except: pass

    def run(self):
        if not SCAN_URL: return
        logger.info("üé¨ –°—Ç–∞—Ä—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...")
        
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            resp = requests.get(SCAN_URL, headers=headers, timeout=30)
            soup = BeautifulSoup(resp.text, 'lxml')
            items = soup.select('[data-marker="item"]')
            
            for item in items:
                try:
                    link_tag = item.select_one('[data-marker="item-title"]')
                    raw_title = link_tag.get('title', '')
                    title_low = raw_title.lower()
                    url = urljoin("https://www.avito.ru", link_tag['href'])
                    
                    if url in self.seen: continue
                    if any(word in title_low for word in BAD_KEYWORDS): continue

                    price = int(item.select_one('[itemprop="price"]')['content'])
                    ram, ssd = extract_specs(title_low)
                    
                    # –°–¢–†–û–ì–ò–ô –ü–û–ò–°–ö –ú–û–î–ï–õ–ò
                    matched_stat = None
                    for (m_name, m_ram, m_ssd), stat in self.prices.items():
                        # –†–∞–∑–±–∏–≤–∞–µ–º –∏–º—è –∏–∑ –±–∞–∑—ã (–Ω–∞–ø—Ä. "macbook air 13 (2020, m1)") –Ω–∞ —Å–ª–æ–≤–∞
                        keywords = re.findall(r'[a-z0-9]+', m_name)
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –í–°–ï —Å–ª–æ–≤–∞ (2020, m1 –∏ —Ç.–¥.) –µ—Å—Ç—å –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –ê–≤–∏—Ç–æ
                        if all(word in title_low for word in keywords):
                            if m_ram == ram and m_ssd == ssd:
                                matched_stat = stat
                                break
                    
                    if matched_stat:
                        market_low = matched_stat['median_price']
                        # –ï—Å–ª–∏ —Ü–µ–Ω–∞ —Ä–µ–∞–ª—å–Ω–æ –≤–∫—É—Å–Ω–∞—è (–Ω–∏–∂–µ —Ç–≤–æ–µ–≥–æ –ø–æ—Ä–æ–≥–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏)
                        if price <= market_low * 0.98:
                            logger.info(f"üî• –ü–æ–ø–∞–¥–∞–Ω–∏–µ! {raw_title} –∑–∞ {price}")
                            
                            cycles, urgent_desc = self.deep_analyze(url)
                            urgent_title = any(word in title_low for word in URGENT_KEYWORDS)
                            
                            date_tag = item.select_one('[data-marker="item-date"]')
                            date_str = date_tag.get_text().strip() if date_tag else "–¢–æ–ª—å–∫–æ —á—Ç–æ"

                            deal = HotDeal(
                                url=url, title=raw_title, price=price,
                                market_low=market_low, buyout=matched_stat['buyout_price'],
                                discount_percent=round((1 - price/market_low)*100, 1),
                                model=matched_stat['model_name'], date=date_str,
                                ram=ram, ssd=ssd, battery_cycles=cycles,
                                is_urgent=(urgent_title or urgent_desc)
                            )
                            self.notify(deal)
                            self.seen.add(url)
                            time.sleep(random.uniform(2, 4)) # –ü–∞—É–∑–∞ –ø–æ—Å–ª–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                except Exception as e:
                    continue

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            SEEN_FILE.parent.mkdir(parents=True, exist_ok=True)
            with open(SEEN_FILE, 'w', encoding='utf-8') as f:
                json.dump({"seen_urls": list(self.seen)[-3000:]}, f)

        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    AvitoScanner().run()
