#!/usr/bin/env python3
import json
import os
import re
import time
import random
import logging
import urllib3
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List, Set, Dict
from pathlib import Path
from urllib.parse import urljoin

# –û—Ç–∫–ª—é—á–∞–µ–º –≤–æ—Ä–Ω–∏–Ω–≥–∏ SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
try:
    import requests
except ImportError:
    requests = None

try:
    from curl_cffi import requests as cffi_requests
    HAS_CFFI = True
except ImportError:
    cffi_requests = requests
    HAS_CFFI = False

from bs4 import BeautifulSoup

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("AvitoScanner")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
SCAN_URL = os.environ.get('SCAN_URL')
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
PROXY_URL = os.environ.get('PROXY_URL')
PROXY_CHANGE_IP_URL = os.environ.get('CHANGE_IP_URL')

HOT_DEAL_THRESHOLD = 0.94 
PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_DEALS_FILE = Path("public/data/seen-hot-deals.json")

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
URGENT_KEYWORDS = ['—Å—Ä–æ—á–Ω–æ', '—Ç–æ—Ä–≥', '—Å–µ–≥–æ–¥–Ω—è', '–ø–µ—Ä–µ–µ–∑–¥', '–æ—Ç–¥–∞—é', '–¥–µ—à–µ–≤–æ', '–±—ã—Å—Ç—Ä–æ']
BAD_KEYWORDS = ['icloud', '–∑–∞–ø—á–∞—Å—Ç–∏', '–±–∏—Ç—ã–π', '—Ä–∞–∑–±–∏—Ç', '–±–ª–æ–∫–∏—Ä–æ–≤', '—ç–∫—Ä–∞–Ω', '–¥–µ—Ñ–µ–∫—Ç', 'mdm', '–∞–∫–∫–∞—É–Ω—Ç', '–∫–æ—Ä–æ–±–∫–∞', '—á–µ—Ö–æ–ª']

@dataclass
class HotDeal:
    url: str
    title: str
    price: int
    median_price: int
    discount_percent: float
    model: str
    date: str
    battery_cycles: Optional[int] = None
    is_urgent: bool = False

class AvitoScanner:
    def __init__(self):
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫–∞–Ω–µ—Ä–∞...")
        self.session = cffi_requests.Session() if HAS_CFFI else requests.Session()
        self.prices_db = self._load_prices()
        self.seen_deals = self._load_seen()

    def _load_prices(self) -> Dict[str, int]:
        if not PRICES_FILE.exists():
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª —Ü–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω: {PRICES_FILE}")
            return {}
        try:
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return {s['model_name']: s['median_price'] for s in data.get('stats', [])}
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ü–µ–Ω: {e}")
            return {}

    def _load_seen(self) -> Set[str]:
        if not SEEN_DEALS_FILE.exists(): return set()
        try:
            with open(SEEN_DEALS_FILE, 'r', encoding='utf-8') as f:
                return set(json.load(f).get('seen_urls', []))
        except: return set()

    def _save_seen(self):
        try:
            SEEN_DEALS_FILE.parent.mkdir(parents=True, exist_ok=True)
            keep_urls = list(self.seen_deals)[-3000:]
            with open(SEEN_DEALS_FILE, 'w', encoding='utf-8') as f:
                json.dump({'updated_at': datetime.now().isoformat(), 'seen_urls': keep_urls}, f)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")

    def _rotate_ip(self):
        if PROXY_CHANGE_IP_URL and requests:
            try:
                logger.info("üîÑ –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–º–µ–Ω—É IP...")
                requests.get(PROXY_CHANGE_IP_URL, timeout=20, verify=False)
                time.sleep(15)
            except Exception as e:
                logger.error(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–º–µ–Ω–∏—Ç—å IP: {e}")

    def get_page(self, url: str) -> Optional[str]:
        proxies = {"http": PROXY_URL, "https": PROXY_URL} if PROXY_URL else None
        for attempt in range(3):
            try:
                if HAS_CFFI:
                    resp = self.session.get(url, impersonate="chrome120", proxies=proxies, timeout=30)
                else:
                    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
                    resp = self.session.get(url, headers=headers, proxies=proxies, timeout=30)
                
                if resp.status_code == 200 and "–¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω" not in resp.text.lower():
                    return resp.text
                
                logger.warning(f"‚ö†Ô∏è –ö–æ–¥ {resp.status_code} –∏–ª–∏ –±–ª–æ–∫ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt+1}")
                self._rotate_ip()
            except Exception as e:
                logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
                self._rotate_ip()
        return None

    def analyze_details(self, url: str):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏–∫–ª–æ–≤ –ê–ö–ë"""
        html = self.get_page(url)
        if not html: return None, False
        
        soup = BeautifulSoup(html, 'lxml')
        desc_block = soup.find('div', attrs={'data-marker': 'item-description'})
        text = desc_block.get_text().lower() if desc_block else ""
        
        # –ü–æ–∏—Å–∫ —Ü–∏–∫–ª–æ–≤
        cycles = None
        match = re.search(r'(\d+)\s*(?:—Ü–∏–∫–ª|cycle|—Ü\.|cyc)', text)
        if match:
            try:
                cycles = int(match.group(1))
            except: pass
            
        urgent = any(word in text for word in URGENT_KEYWORDS)
        return cycles, urgent

    def extract_model(self, title: str) -> Optional[str]:
        t = title.lower()
        if any(word in t for word in BAD_KEYWORDS): return None
        
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–∑ –±–∞–∑—ã —Ü–µ–Ω
        for model_name in self.prices_db.keys():
            # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
            clean_name = model_name.split('(')[0].strip().lower()
            if clean_name in t:
                return model_name
        return None

    def run(self):
        logger.info("üé¨ –ù–∞—á–∞–ª–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...")
        html = self.get_page(SCAN_URL)
        if not html:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return
        
        soup = BeautifulSoup(html, 'lxml')
        blocks = soup.find_all('div', attrs={'data-marker': 'item'})
        logger.info(f"üì¶ –ù–∞–π–¥–µ–Ω–æ –±–ª–æ–∫–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(blocks)}")
        
        for block in blocks:
            try:
                link_tag = block.find('a', attrs={'data-marker': 'item-title'})
                if not link_tag: continue
                
                url = urljoin("https://www.avito.ru", link_tag.get('href', ''))
                if url in self.seen_deals: continue

                title = link_tag.get('title', '').strip()
                model = self.extract_model(title)
                
                if not model: continue
                
                price_tag = block.find('meta', attrs={'itemprop': 'price'})
                price = int(price_tag.get('content', 0)) if price_tag else 0
                
                median = self.prices_db.get(model)
                if median and price > (median * 0.3) and price <= (median * HOT_DEAL_THRESHOLD):
                    logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ: {title} –∑–∞ {price}")
                    
                    # –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
                    cycles, urgent_desc = self.analyze_details(url)
                    urgent_title = any(word in title.lower() for word in URGENT_KEYWORDS)
                    
                    # –î–∞—Ç–∞
                    date_tag = block.find('p', attrs={'data-marker': 'item-date'})
                    date_str = date_tag.get_text().strip() if date_tag else "–ù–µ–¥–∞–≤–Ω–æ"

                    deal = HotDeal(
                        url=url, title=title, price=price, median_price=median,
                        discount_percent=round((1 - price/median)*100, 1),
                        model=model, date=date_str, battery_cycles=cycles,
                        is_urgent=(urgent_title or urgent_desc)
                    )
                    
                    self.notify(deal)
                    self.seen_deals.add(url)
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ —á—Ç–æ–±—ã –Ω–µ —á–∞—Å—Ç–∏—Ç—å –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤–≥–ª—É–±—å
                    time.sleep(random.uniform(2, 5))
            except Exception as e:
                logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –±–ª–æ–∫–∞: {e}")
                continue
                
        self._save_seen()
        logger.info("üèÅ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    def notify(self, d: HotDeal):
        if not TELEGRAM_URL or not requests: return
        
        icons = ""
        if d.is_urgent: icons += "üö® –°–†–û–ß–ù–û! "
        if d.battery_cycles and d.battery_cycles < 150: icons += "üîã –ê–ö–ë –•–û–†–û–®–ò–ô! "
        
        text = (
            f"{icons}\n"
            f"üî• <b>{d.model}</b>\n"
            f"üí∞ –¶–µ–Ω–∞: <b>{d.price:,} ‚ÇΩ</b>\n"
            f"üìâ –í—ã–≥–æ–¥–∞: {d.discount_percent}% (–†—ã–Ω–æ–∫: {d.median_price:,})\n"
            f"‚ö° –ê–ö–ë: {f'{d.battery_cycles} —Ü–∏–∫–ª–æ–≤' if d.battery_cycles else '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}\n"
            f"üïí –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {d.date}\n"
            f"üîó <a href='{d.url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ Avito</a>"
        )
        try:
            requests.post(TELEGRAM_URL, json={"text": text, "parse_mode": "HTML"}, timeout=10)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ TG: {e}")

if __name__ == "__main__":
    try:
        scanner = AvitoScanner()
        scanner.run()
    except Exception as e:
        logger.error(f"üí• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
