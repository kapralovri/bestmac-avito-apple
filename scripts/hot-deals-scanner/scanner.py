#!/usr/bin/env python3
import json
import os
import re
import time
import random
import logging
import urllib3
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List, Set, Dict
from pathlib import Path
from urllib.parse import urljoin

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: pip install requests beautifulsoup4 lxml")
    exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("Scanner")

PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_FILE = Path("public/data/seen-hot-deals.json")
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
SCAN_URL = os.environ.get('SCAN_URL')

def extract_specs(text: str):
    text = text.lower().replace(' ', '')
    ram = 8
    ram_match = re.search(r'\b(8|16|18|24|32|36|48|64|96|128)(?:gb|–≥–±)\b', text)
    if ram_match: ram = int(ram_match.group(1))
    
    ssd = 256
    ssd_matches = re.findall(r'(\d+)(?:gb|–≥–±|tb|—Ç–±)', text)
    for val_str in ssd_matches:
        val = int(val_str)
        if 2010 <= val <= 2026: continue 
        if val in [128, 256, 512, 1, 2, 4]:
            ssd = val * 1024 if val <= 8 else val
            break
    return ram, ssd

class AvitoScanner:
    def __init__(self):
        self.prices = {}
        if PRICES_FILE.exists():
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                stats = data.get('stats', [])
                logger.info(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã —Ü–µ–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(stats)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π")
                for s in stats:
                    name = s['model_name'].lower()
                    self.prices[(name, s['ram'], s['ssd'])] = s
        else:
            logger.error(f"‚ùå –§–ê–ô–õ –ë–ê–ó–´ –¶–ï–ù –ù–ï –ù–ê–ô–î–ï–ù: {PRICES_FILE}")

        self.seen = set()
        if SEEN_FILE.exists():
            try:
                with open(SEEN_FILE, 'r', encoding='utf-8') as f:
                    self.seen = set(json.load(f).get('seen_urls', []))
            except: pass

    def deep_analyze(self, url: str):
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"}
            resp = requests.get(url, headers=headers, timeout=15, verify=False)
            if resp.status_code != 200: return None, False
            soup = BeautifulSoup(resp.text, 'lxml')
            desc = soup.find('div', attrs={'data-marker': 'item-description'})
            text = desc.get_text().lower() if desc else ""
            cycles = None
            c_match = re.search(r'(\d+)\s*(?:—Ü–∏–∫–ª|cycle|—Ü\.|cyc)', text)
            if c_match: cycles = int(c_match.group(1))
            urgent = any(word in text for word in ['—Å—Ä–æ—á–Ω–æ', '—Ç–æ—Ä–≥', '—Å–µ–≥–æ–¥–Ω—è', '–ø–µ—Ä–µ–µ–∑–¥'])
            return cycles, urgent
        except: return None, False

    def notify(self, title, price, market_low, buyout, ram, ssd, url, cycles, urgent):
        if not TELEGRAM_URL: return
        status = "üö® <b>–°–†–û–ß–ù–û!</b> " if urgent else ""
        if cycles and cycles < 150: status += "üîã <b>–ê–ö–ë –ò–î–ï–ê–õ!</b>"
        
        text = (
            f"üéØ <b>–ù–∞—à–µ–ª –ø–æ –ù–ò–ó–£ —Ä—ã–Ω–∫–∞!</b>\n{status}\n\n"
            f"üíª {title}\n"
            f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥: <b>{ram}GB / {ssd}GB</b>\n"
            f"üí∞ –¶–µ–Ω–∞: <b>{price:,} ‚ÇΩ</b>\n"
            f"üìâ –ù–∏–∑ —Ä—ã–Ω–∫–∞: {market_low:,} ‚ÇΩ\n"
            f"ü§ù –¢–≤–æ–π –≤—ã–∫—É–ø: {buyout:,} ‚ÇΩ\n"
            f"‚ö° –¶–∏–∫–ª—ã: {cycles if cycles else '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}\n"
            f"üîó <a href='{url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ Avito</a>"
        )
        requests.post(TELEGRAM_URL, json={"text": text, "parse_mode": "HTML"}, timeout=10)

    def run(self):
        if not SCAN_URL:
            logger.error("‚ùå SCAN_URL –Ω–µ –∑–∞–¥–∞–Ω!")
            return
        
        logger.info(f"üé¨ –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            resp = requests.get(SCAN_URL, headers=headers, timeout=30, verify=False)
            
            if resp.status_code != 200:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {resp.status_code}")
                return

            soup = BeautifulSoup(resp.text, 'lxml')
            items = soup.select('[data-marker="item"]')
            logger.info(f"üîé –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(items)}")
            
            matches_count = 0
            for item in items:
                try:
                    link_tag = item.select_one('[data-marker="item-title"]')
                    title = link_tag.get('title', '').lower()
                    url = urljoin("https://www.avito.ru", link_tag['href'])
                    
                    if url in self.seen: continue
                    
                    price_tag = item.select_one('[itemprop="price"]')
                    if not price_tag: continue
                    price = int(price_tag['content'])
                    
                    ram, ssd = extract_specs(title)
                    
                    # –ü–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    for (m_name, m_ram, m_ssd), stat in self.prices.items():
                        keywords = re.findall(r'[a-z0-9]+', m_name)
                        if all(word in title for word in keywords) and m_ram == ram and m_ssd == ssd:
                            market_low = stat['median_price']
                            # –ï—Å–ª–∏ —Ü–µ–Ω–∞ —Ä–µ–∞–ª—å–Ω–æ –Ω–∏–∂–µ –Ω–∏–∑–∞ —Ä—ã–Ω–∫–∞
                            if price <= market_low * 0.98:
                                logger.info(f"üî• MATCH: {title} –∑–∞ {price} (–ù–∏–∑: {market_low})")
                                cycles, urgent_desc = self.deep_analyze(url)
                                self.notify(title, price, market_low, stat['buyout_price'], ram, ssd, url, cycles, urgent_desc)
                                self.seen.add(url)
                                matches_count += 1
                                time.sleep(2)
                            break
                except Exception as e: continue
            
            logger.info(f"üèÅ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–∫–æ–Ω—á–µ–Ω–æ. –ù–æ–≤—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {matches_count}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            if matches_count > 0:
                SEEN_FILE.parent.mkdir(parents=True, exist_ok=True)
                with open(SEEN_FILE, 'w', encoding='utf-8') as f:
                    json.dump({"seen_urls": list(self.seen)[-3000:]}, f)

        except Exception as e:
            logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    AvitoScanner().run()
