#!/usr/bin/env python3
import json
import os
import re
import time
import random
import logging
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Optional, List, Set, Dict
from pathlib import Path
from urllib.parse import urljoin

try:
    from curl_cffi import requests as cffi_requests
    HAS_CFFI = True
except ImportError:
    import requests as cffi_requests
    HAS_CFFI = False

from bs4 import BeautifulSoup

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("AvitoScanner")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
SCAN_URL = os.environ.get('SCAN_URL')
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
PROXY_URL = os.environ.get('PROXY_URL')
PROXY_CHANGE_IP_URL = os.environ.get('CHANGE_IP_URL') 

HOT_DEAL_THRESHOLD = 0.90
PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_DEALS_FILE = Path("public/data/seen-hot-deals.json")

IMPERSONATE = "chrome120"
TIMEOUT = 30
MAX_RETRIES = 3

@dataclass
class HotDeal:
    url: str
    title: str
    price: int
    median_price: int
    discount_percent: float
    model: str
    found_at: str

class AvitoScanner:
    def __init__(self):
        self.session = cffi_requests.Session()
        self.prices_db = self._load_prices()
        self.seen_deals = self._load_seen()
        
    def _load_prices(self) -> Dict[str, int]:
        if not PRICES_FILE.exists():
            logger.warning(f"‚ö†Ô∏è –ë–∞–∑–∞ —Ü–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {PRICES_FILE}")
            return {}
        try:
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            prices = {}
            for stat in data.get('stats', []):
                name = stat.get('model_name')
                median = stat.get('median_price')
                if name and median:
                    prices[name] = median
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(prices)} –º–æ–¥–µ–ª–µ–π —Ü–µ–Ω")
            return prices
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –±–∞–∑—ã —Ü–µ–Ω: {e}")
            return {}

    def _load_seen(self) -> Set[str]:
        if not SEEN_DEALS_FILE.exists():
            return set()
        try:
            with open(SEEN_DEALS_FILE, 'r', encoding='utf-8') as f:
                content = json.load(f)
                return set(content.get('seen_urls', []))
        except Exception:
            return set()

    def _save_seen(self):
        try:
            SEEN_DEALS_FILE.parent.mkdir(parents=True, exist_ok=True)
            # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2000 —Å—Å—ã–ª–æ–∫
            list_urls = list(self.seen_deals)
            keep_urls = list_urls[-2000:]
            with open(SEEN_DEALS_FILE, 'w', encoding='utf-8') as f:
                json.dump({
                    'updated_at': datetime.now().isoformat(), 
                    'seen_urls': keep_urls
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")

    def _rotate_ip(self):
        if PROXY_CHANGE_IP_URL:
            try:
                logger.info("üîÑ –í—ã–∑–æ–≤ API —Å–º–µ–Ω—ã IP...")
                import requests
                # –°–º–µ–Ω–∞ IP –¥–µ–ª–∞–µ—Ç—Å—è –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –ø—Ä—è–º—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
                requests.get(PROXY_CHANGE_IP_URL, timeout=15)
                logger.info("‚è≥ –ü–∞—É–∑–∞ 15 —Å–µ–∫ –¥–ª—è —Å–º–µ–Ω—ã IP...")
                time.sleep(15) 
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–º–µ–Ω–µ IP: {e}")
                time.sleep(10)
        else:
            logger.warning("‚è≥ –°—Å—ã–ª–∫–∞ –¥–ª—è —Å–º–µ–Ω—ã IP –Ω–µ –∑–∞–¥–∞–Ω–∞, –ø—Ä–æ—Å—Ç–æ –∂–¥–µ–º...")
            time.sleep(20)

    def get_page(self, url: str) -> Optional[str]:
        proxies = {"http": PROXY_URL, "https": PROXY_URL} if PROXY_URL else None
        
        for attempt in range(MAX_RETRIES):
            try:
                if HAS_CFFI:
                    resp = self.session.get(
                        url, 
                        impersonate=IMPERSONATE, 
                        proxies=proxies, 
                        timeout=TIMEOUT,
                        allow_redirects=True
                    )
                else:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Accept-Language': 'ru-RU,ru;q=0.9',
                    }
                    resp = self.session.get(url, headers=headers, proxies=proxies, timeout=TIMEOUT)

                if resp.status_code == 200:
                    if "firewall" in resp.text.lower() or "–¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω" in resp.text.lower():
                        logger.warning("‚ö†Ô∏è –ö–∞–ø—á–∞/–ë–ª–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω. –ú–µ–Ω—è–µ–º IP...")
                        self._rotate_ip()
                        continue
                    return resp.text
                
                elif resp.status_code in [403, 429]:
                    logger.warning(f"‚ö†Ô∏è –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ {resp.status_code}. –ú–µ–Ω—è–µ–º IP...")
                    self._rotate_ip()
                    continue
                else:
                    logger.error(f"HTTP Error {resp.status_code}")
                    self._rotate_ip()
            
            except Exception as e:
                logger.error(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞: {e}")
                self._rotate_ip()
        
        return None

    def parse_listings(self, html: str) -> List[dict]:
        soup = BeautifulSoup(html, 'lxml')
        items = []
        listing_blocks = soup.find_all('div', attrs={'data-marker': 'item'})
        
        for block in listing_blocks:
            try:
                link_tag = block.find('a', attrs={'data-marker': 'item-title'})
                if not link_tag: continue
                    
                title = link_tag.get('title', '').strip()
                url = urljoin("https://www.avito.ru", link_tag.get('href', ''))
                
                price_meta = block.find('meta', attrs={'itemprop': 'price'})
                if price_meta:
                    price = int(price_meta.get('content', 0))
                else:
                    price_tag = block.find('p', attrs={'data-marker': 'item-price'})
                    price_text = price_tag.get_text() if price_tag else "0"
                    price = int(re.sub(r'\D', '', price_text) or 0)

                if price < 5000: continue
                items.append({'url': url, 'title': title, 'price': price})
            except: continue
                
        logger.info(f"üì¶ –†–∞—Å–ø–∞—Ä—Å–µ–Ω–æ {len(items)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        return items

    def extract_model(self, title: str) -> Optional[str]:
        t = title.lower()
        patterns = [
            (r'macbook\s*pro\s*16.*m4\s*max', 'MacBook Pro 16 (2024, M4 Max)'),
            (r'macbook\s*pro\s*16.*m4\s*pro', 'MacBook Pro 16 (2024, M4 Pro)'),
            (r'macbook\s*pro\s*16.*m3\s*max', 'MacBook Pro 16 (2023, M3 Max)'),
            (r'macbook\s*pro\s*16.*m3\s*pro', 'MacBook Pro 16 (2023, M3 Pro)'),
            (r'macbook\s*pro\s*16.*m2\s*max', 'MacBook Pro 16 (2023, M2 Max)'),
            (r'macbook\s*pro\s*16.*m2\s*pro', 'MacBook Pro 16 (2023, M2 Pro)'),
            (r'macbook\s*pro\s*16.*m1\s*max', 'MacBook Pro 16 (2021, M1 Max)'),
            (r'macbook\s*pro\s*16.*m1\s*pro', 'MacBook Pro 16 (2021, M1 Pro)'),
            (r'macbook\s*pro\s*14.*m3\s*max', 'MacBook Pro 14 (2023, M3 Max)'),
            (r'macbook\s*pro\s*14.*m3\s*pro', 'MacBook Pro 14 (2023, M3 Pro)'),
            (r'macbook\s*pro\s*14.*m3', 'MacBook Pro 14 (2023, M3)'),
            (r'macbook\s*pro\s*14.*m2', 'MacBook Pro 14 (2023, M2)'),
            (r'macbook\s*pro\s*14.*m1', 'MacBook Pro 14 (2021, M1)'),
            (r'macbook\s*pro\s*13.*m2', 'MacBook Pro 13 (2022, M2)'),
            (r'macbook\s*pro\s*13.*m1', 'MacBook Pro 13 (2020, M1)'),
            (r'macbook\s*air\s*15.*m3', 'MacBook Air 15 (2024, M3)'),
            (r'macbook\s*air\s*15.*m2', 'MacBook Air 15 (2023, M2)'),
            (r'macbook\s*air\s*13.*m3', 'MacBook Air 13 (2024, M3)'),
            (r'macbook\s*air\s*13.*m2', 'MacBook Air 13 (2022, M2)'),
            (r'macbook\s*air.*m1', 'MacBook Air 13 (2020, M1)'),
        ]
        for pattern, model in patterns:
            if re.search(pattern, t): return model
        return None

    def find_deals(self, listings: List[dict]) -> List[HotDeal]:
        deals = []
        for item in listings:
            if item['url'] in self.seen_deals: continue
            model = self.extract_model(item['title'])
            if not model: continue
            median = self.prices_db.get(model)
            if not median: continue
            if item['price'] < (median * 0.4): continue
            
            if item['price'] <= (median * HOT_DEAL_THRESHOLD):
                discount = (1 - item['price'] / median) * 100
                deals.append(HotDeal(
                    url=item['url'], title=item['title'], price=item['price'],
                    median_price=median, discount_percent=round(discount, 1),
                    model=model, found_at=datetime.now().isoformat()
                ))
        return deals

    def send_notifications(self, deals: List[HotDeal]):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –≤ Telegram"""
        if not deals or not TELEGRAM_URL: 
            logger.warning("‚ö†Ô∏è –ù–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∏–ª–∏ URL –ø—É—Å—Ç")
            return

        import requests
        for deal in deals:
            try:
                # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, —á—Ç–æ–±—ã HTML –Ω–µ –ª–æ–º–∞–ª—Å—è
                safe_title = deal.title.replace('<', '&lt;').replace('>', '&gt;').replace('&', '&amp;')
                
                text = (
                    f"üî• <b>HOT DEAL: {deal.model}</b>\n"
                    f"üí∞ –¶–µ–Ω–∞: <b>{deal.price:,} ‚ÇΩ</b>\n"
                    f"üìâ –ú–µ–¥–∏–∞–Ω–∞: {deal.median_price:,} ‚ÇΩ (–í—ã–≥–æ–¥–∞ {deal.discount_percent}%)\n"
                    f"üîó <a href='{deal.url}'>–û—Ç–∫—Ä—ã—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ</a>"
                )
                
                payload = {
                    "text": text,
                    "parse_mode": "HTML",
                    "disable_web_page_preview": False
                }
                
                resp = requests.post(TELEGRAM_URL, json=payload, timeout=15)
                
                if resp.status_code != 200:
                    logger.error(f"‚ùå Telegram Error {resp.status_code}: {resp.text}")
                else:
                    logger.info(f"‚úÖ –î–æ—Å—Ç–∞–≤–ª–µ–Ω–æ: {deal.model}")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")

    def run(self):
        if not self.prices_db:
            logger.error("‚ùå –ë–∞–∑–∞ —Ü–µ–Ω –ø—É—Å—Ç–∞. –°–∫—Ä–∏–ø—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
            return

        logger.info("üé¨ –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...")
        html = self.get_page(SCAN_URL)
        if html:
            listings = self.parse_listings(html)
            hot_deals = self.find_deals(listings)
            
            if hot_deals:
                logger.info(f"üî• –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å–¥–µ–ª–æ–∫: {len(hot_deals)}")
                self.send_notifications(hot_deals)
                for d in hot_deals:
                    self.seen_deals.add(d.url)
                self._save_seen()
            else:
                logger.info("ü§∑ –ù–∏—á–µ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å Avito")

if __name__ == "__main__":
    AvitoScanner().run()
