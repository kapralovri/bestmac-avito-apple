#!/usr/bin/env python3
import json
import os
import re
import time
import random
import logging
import urllib3
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: pip install requests beautifulsoup4 lxml")
    exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("Scanner")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_FILE = Path("public/data/seen-hot-deals.json")
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
SCAN_URL = os.environ.get('SCAN_URL')
PROXY_URL = os.environ.get('PROXY_URL', '').strip().strip('"').strip("'")
CHANGE_IP_URL = os.environ.get('CHANGE_IP_URL', '').strip().strip('"').strip("'")

# –ü–æ—Ä–æ–≥: 10% –æ—Ç –Ω–∏–∑–∞ —Ä—ã–Ω–∫–∞
PRICE_THRESHOLD_FACTOR = 1.10 

def clean_url(url: str) -> str:
    return url.split('?')[0]

def extract_specs(text: str):
    text = text.lower().replace(' ', '')
    matches = re.findall(r'(\d+)(?:gb|–≥–±|tb|—Ç–±)', text)
    ram, ssd = 8, 256
    clean_matches = [m for m in matches if not (2018 <= int(m) <= 2026)]
    if len(clean_matches) >= 2:
        ram = int(clean_matches[0])
        ssd_val = int(clean_matches[1])
        ssd = ssd_val * 1024 if ssd_val <= 8 else ssd_val
    elif len(clean_matches) == 1:
        val = int(clean_matches[0])
        if val in [8, 16, 18, 24, 32, 36, 48, 64]: ram = val
        else: ssd = val
    return ram, ssd

class AvitoScanner:
    def __init__(self):
        p_str = PROXY_URL
        if p_str and not p_str.startswith('http'):
            p_str = f"http://{p_str}"
        self.proxies = {"http": p_str, "https": p_str} if p_str else None
        
        self.prices = {}
        if PRICES_FILE.exists():
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                stats = data.get('stats', [])
                for s in stats:
                    self.prices[(s['model_name'].lower(), int(s['ram']), int(s['ssd']))] = s
                logger.info(f"üìä –ë–∞–∑–∞ —Ü–µ–Ω –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(self.prices)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π")
        else:
            logger.error("‚ùå –§–∞–π–ª avito-prices.json –Ω–µ –Ω–∞–π–¥–µ–Ω!")

        self.seen = set()
        if SEEN_FILE.exists():
            try:
                with open(SEEN_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.seen = set(clean_url(u) for u in data.get('seen_urls', []))
            except: pass

    def rotate_ip(self):
        if CHANGE_IP_URL:
            try:
                logger.info("üîÑ –°–º–µ–Ω–∞ IP...")
                requests.get(CHANGE_IP_URL, timeout=15, verify=False)
                time.sleep(15)
            except: pass

    def get_with_retry(self, url):
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        for attempt in range(3):
            try:
                resp = requests.get(url, headers=headers, proxies=self.proxies, timeout=30, verify=False)
                if resp.status_code == 200: return resp
                if resp.status_code in [403, 429]: self.rotate_ip()
            except:
                self.rotate_ip()
                time.sleep(5)
        return None

    def run(self):
        if not SCAN_URL: return
        logger.info("üé¨ –ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...")
        
        resp = self.get_with_retry(SCAN_URL)
        if not resp:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å SCAN_URL")
            return

        soup = BeautifulSoup(resp.text, 'lxml')
        items = soup.select('[data-marker="item"]')
        
        total_found = len(items)
        model_matches = 0
        price_matches = 0
        new_ads_saved = 0

        for item in items:
            try:
                link_tag = item.select_one('[data-marker="item-title"]')
                raw_url = urljoin("https://www.avito.ru", link_tag['href'])
                url = clean_url(raw_url)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏
                if url in self.seen: continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ–π–¥–∂ "–ù–∏–∂–µ —Ä—ã–Ω–æ—á–Ω–æ–π" (–∏—â–µ–º —Ç–µ–∫—Å—Ç –≤–æ –≤—Å–µ–º –±–ª–æ–∫–µ)
                full_text = item.get_text().lower()
                is_avito_low = any(x in full_text for x in ["–Ω–∏–∂–µ —Ä—ã–Ω–æ—á–Ω–æ–π", "—Ü–µ–Ω–∞ –Ω–∏–∂–µ", "—Ö–æ—Ä–æ—à–∞—è —Ü–µ–Ω–∞"])

                price_tag = item.select_one('[itemprop="price"]')
                price = int(price_tag['content']) if price_tag else 0
                if price < 15000: continue

                raw_title = link_tag.get('title', '')
                ram, ssd = extract_specs(raw_title.lower())
                
                # –ò—â–µ–º –º–æ–¥–µ–ª—å –≤ –±–∞–∑–µ
                matched_stat = None
                for (m_name, m_ram, m_ssd), stat in self.prices.items():
                    keywords = re.findall(r'[a-z0-9]+', m_name)
                    if all(word in raw_title.lower() for word in keywords) and m_ram == ram and m_ssd == ssd:
                        matched_stat = stat
                        break
                
                if matched_stat:
                    model_matches += 1
                    market_low = matched_stat['min_price']
                    
                    # –õ–æ–≥–∏–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
                    is_match = False
                    if price <= int(market_low * PRICE_THRESHOLD_FACTOR): is_match = True
                    if is_avito_low: is_match = True # –ë–µ–π–¥–∂ –ê–≤–∏—Ç–æ ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç

                    if is_match:
                        price_matches += 1
                        logger.info(f"üî• –ü–æ–ø–∞–¥–∞–Ω–∏–µ! {price} —Ä—É–±. (Badge: {is_avito_low})")
                        
                        # –°–æ–æ–±—â–µ–Ω–∏–µ –≤ TG
                        badge_status = "üìâ <b>–ê–≤–∏—Ç–æ: –ù–∏–∂–µ —Ä—ã–Ω–∫–∞!</b>\n" if is_avito_low else ""
                        text = (
                            f"üéØ <b>–ù–∞—à–µ–ª –≤–∞—Ä–∏–∞–Ω—Ç!</b>\n{badge_status}\n"
                            f"üíª {raw_title}\n"
                            f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥: <b>{ram}GB / {ssd}GB</b>\n"
                            f"üí∞ –¶–µ–Ω–∞: <b>{price:,} ‚ÇΩ</b>\n"
                            f"üìâ –ù–∏–∑ —Ä—ã–Ω–∫–∞: {market_low:,} ‚ÇΩ\n"
                            f"ü§ù –¢–≤–æ–π –≤—ã–∫—É–ø: {matched_stat['buyout_price']:,} ‚ÇΩ\n"
                            f"üîó <a href='{url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ Avito</a>"
                        )
                        requests.post(TELEGRAM_URL, json={"text": text, "parse_mode": "HTML"}, timeout=10, proxies=None)
                        
                        self.seen.add(url)
                        new_ads_saved += 1
            except: continue

        # –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –í –õ–û–ì–ò
        logger.info(f"üèÅ –ò—Ç–æ–≥: –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {total_found} –æ–±—ä—è–≤–ª–µ–Ω–∏–π.")
        logger.info(f"   - –°–æ–≤–ø–∞–ª–æ –º–æ–¥–µ–ª–µ–π: {model_matches}")
        logger.info(f"   - –ü–æ–¥–æ—à–ª–æ –ø–æ —Ü–µ–Ω–µ: {price_matches}")

        if price_matches == 0:
            logger.info("ü§∑ –ù–∏—á–µ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–≥–æ –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

        if new_ads_saved > 0:
            with open(SEEN_FILE, 'w', encoding='utf-8') as f:
                json.dump({"updated_at": datetime.now().isoformat(), "seen_urls": list(self.seen)[-4000:]}, f)

if __name__ == "__main__":
    AvitoScanner().run()
