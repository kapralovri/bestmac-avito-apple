#!/usr/bin/env python3
import json
import os
import re
import time
import random
import logging
import urllib3
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List, Set, Dict
from pathlib import Path
from urllib.parse import urljoin

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    from curl_cffi import requests as cffi_requests
    HAS_CFFI = True
except ImportError:
    import requests as cffi_requests
    HAS_CFFI = False

from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("AvitoScanner")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
SCAN_URL = os.environ.get('SCAN_URL')
TELEGRAM_URL = os.environ.get('TELEGRAM_NOTIFY_URL')
PROXY_URL = os.environ.get('PROXY_URL')
PROXY_CHANGE_IP_URL = os.environ.get('CHANGE_IP_URL')

HOT_DEAL_THRESHOLD = 0.94 
PRICES_FILE = Path("public/data/avito-prices.json")
SEEN_DEALS_FILE = Path("public/data/seen-hot-deals.json")

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
URGENT_KEYWORDS = ['—Å—Ä–æ—á–Ω–æ', '—Ç–æ—Ä–≥', '—Å–µ–≥–æ–¥–Ω—è', '–ø–µ—Ä–µ–µ–∑–¥', '–æ—Ç–¥–∞—é', '–¥–µ—à–µ–≤–æ']
BAD_KEYWORDS = ['icloud', '–∑–∞–ø—á–∞—Å—Ç–∏', '–±–∏—Ç—ã–π', '—Ä–∞–∑–±–∏—Ç', '–±–ª–æ–∫–∏—Ä–æ–≤', '—ç–∫—Ä–∞–Ω', '–¥–µ—Ñ–µ–∫—Ç', 'mdm', '–∞–∫–∫–∞—É–Ω—Ç']

@dataclass
class HotDeal:
    url: str
    title: str
    price: int
    median_price: int
    discount_percent: float
    model: str
    date: str
    battery_cycles: Optional[int] = None
    is_urgent: bool = False

class AvitoScanner:
    def __init__(self):
        self.session = cffi_requests.Session()
        self.prices_db = self._load_prices()
        self.seen_deals = self._load_seen()

    def _load_prices(self):
        if not PRICES_FILE.exists(): return {}
        try:
            with open(PRICES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return {s['model_name']: s['median_price'] for s in data.get('stats', [])}
        except: return {}

    def _load_seen(self):
        if not SEEN_DEALS_FILE.exists(): return set()
        try:
            with open(SEEN_DEALS_FILE, 'r', encoding='utf-8') as f:
                return set(json.load(f).get('seen_urls', []))
        except: return set()

    def _save_seen(self):
        SEEN_DEALS_FILE.parent.mkdir(parents=True, exist_ok=True)
        keep_urls = list(self.seen_deals)[-3000:]
        with open(SEEN_DEALS_FILE, 'w', encoding='utf-8') as f:
            json.dump({'updated_at': datetime.now().isoformat(), 'seen_urls': keep_urls}, f)

    def _rotate_ip(self):
        if PROXY_CHANGE_IP_URL:
            try:
                import requests
                requests.get(PROXY_CHANGE_IP_URL, timeout=20, verify=False)
                time.sleep(15)
            except: pass

    def get_page(self, url: str):
        proxies = {"http": PROXY_URL, "https": PROXY_URL} if PROXY_URL else None
        for _ in range(3):
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º impersonate –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã
                resp = self.session.get(url, impersonate="chrome120", proxies=proxies, timeout=30)
                if resp.status_code == 200: return resp.text
                self._rotate_ip()
            except: self._rotate_ip()
        return None

    def analyze_description(self, url: str):
        """–ó–∞—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç–π –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ê–ö–ë –∏ –°—Ä–æ—á–Ω–æ—Å—Ç–∏"""
        html = self.get_page(url)
        if not html: return None, False
        
        soup = BeautifulSoup(html, 'lxml')
        desc = soup.find('div', attrs={'data-marker': 'item-description'})
        text = desc.get_text().lower() if desc else ""
        
        # –ü–æ–∏—Å–∫ —Ü–∏–∫–ª–æ–≤ –ê–ö–ë (–∏—â–µ–º —Ü–∏—Ñ—Ä—ã —Ä—è–¥–æ–º —Å–æ —Å–ª–æ–≤–∞–º–∏ —Ü–∏–∫–ª, cycle, –∞–∫–±)
        cycles = None
        cycles_match = re.search(r'(\d+)\s*(?:—Ü–∏–∫–ª|cycle|—Ü\.|cyc)', text)
        if cycles_match:
            cycles = int(cycles_match.group(1))
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ä–æ—á–Ω–æ—Å—Ç—å –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
        is_urgent = any(word in text for word in URGENT_KEYWORDS)
        
        return cycles, is_urgent

    def extract_model(self, title: str):
        t = title.lower()
        if any(word in t for word in BAD_KEYWORDS): return None
        patterns = [
            (r'16.*m([1-4])\s*(max|pro)?', 'MacBook Pro 16'),
            (r'14.*m([1-4])\s*(max|pro)?', 'MacBook Pro 14'),
            (r'15.*m([2-3])', 'MacBook Air 15'),
            (r'13.*m([1-3])', 'MacBook Air 13'),
            (r'air.*m1', 'MacBook Air 13 (2020, M1)'),
        ]
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –≤–µ—Ä–Ω–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –ø–æ–ª–Ω—É—é –∏–∑ —Ç–≤–æ–µ–≥–æ —Å–ø–∏—Å–∫–∞
        for pattern, model in patterns:
            if re.search(pattern, t): return model
        return None

    def run(self):
        html = self.get_page(SCAN_URL)
        if not html: return
        
        soup = BeautifulSoup(html, 'lxml')
        blocks = soup.find_all('div', attrs={'data-marker': 'item'})
        
        for block in blocks:
            try:
                link_tag = block.find('a', attrs={'data-marker': 'item-title'})
                url = urljoin("https://www.avito.ru", link_tag.get('href', ''))
                if url in self.seen_deals: continue

                title = link_tag.get('title', '').strip()
                model = self.extract_model(title)
                if not model: continue

                price_tag = block.find('meta', attrs={'itemprop': 'price'})
                price = int(price_tag.get('content', 0)) if price_tag else 0
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ–¥–∏–∞–Ω—É
                median = None
                # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ –ë–î —Ü–µ–Ω
                for m_name, m_price in self.prices_db.items():
                    if m_name.lower() in title.lower():
                        median = m_price
                        model = m_name
                        break
                
                if median and price > (median * 0.4) and price <= (median * HOT_DEAL_THRESHOLD):
                    # –ì–û–†–Ø–ß–û! –î–µ–ª–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                    logger.info(f"üîé –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑: {title}")
                    cycles, is_urgent_desc = self.analyze_description(url)
                    
                    is_urgent_title = any(word in title.lower() for word in URGENT_KEYWORDS)
                    discount = round((1 - price / median) * 100, 1)
                    
                    deal = HotDeal(
                        url=url, title=title, price=price, median_price=median,
                        discount_percent=discount, model=model, date="–¢–æ–ª—å–∫–æ —á—Ç–æ",
                        battery_cycles=cycles, is_urgent=(is_urgent_title or is_urgent_desc)
                    )
                    self.notify(deal)
                    self.seen_deals.add(url)
            except Exception as e:
                continue
        self._save_seen()

    def notify(self, d: HotDeal):
        if not TELEGRAM_URL: return
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–Ω–∞—á–∫–∏
        status_icons = ""
        if d.is_urgent: status_icons += "üö® –°–†–û–ß–ù–û! "
        if d.battery_cycles and d.battery_cycles < 100: status_icons += "üîã –ê–ö–ë –ò–î–ï–ê–õ! "
        
        text = (
            f"{status_icons}\n"
            f"üî• <b>{d.model}</b>\n"
            f"üí∞ –¶–µ–Ω–∞: <b>{d.price:,} ‚ÇΩ</b>\n"
            f"üìâ –í—ã–≥–æ–¥–∞: {d.discount_percent}% (–†—ã–Ω–æ–∫: {d.median_price:,})\n"
            f"‚ö° –ê–ö–ë: {f'{d.battery_cycles} —Ü–∏–∫–ª–æ–≤' if d.battery_cycles else '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}\n"
            f"üîó <a href='{d.url}'>–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ Avito</a>"
        )
        import requests
        requests.post(TELEGRAM_URL, json={"text": text, "parse_mode": "HTML"})

if __name__ == "__main__":
    AvitoScanner().run()
