#!/usr/bin/env python3
import json
import os
import re
import time
import random
import statistics
import argparse
import logging
import urllib3
from pathlib import Path
from dataclasses import dataclass, asdict

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("‚ùå –û—à–∏–±–∫–∞: pip install requests beautifulsoup4 lxml")
    exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("Parser")

# –ü—É—Ç–∏
SCRIPT_DIR = Path(__file__).parent
URLS_FILE = SCRIPT_DIR / "../../public/data/avito-urls.json"
OUTPUT_FILE = SCRIPT_DIR / "../../public/data/avito-prices.json"

# --- –û–ë–†–ê–ë–û–¢–ö–ê –ü–†–û–ö–°–ò (–¢–≤–æ–π —Ñ–æ—Ä–º–∞—Ç login:pass@host:port) ---
RAW_PROXY = os.environ.get("PROXY_URL", "").strip().strip('"').strip("'")
CHANGE_IP_URL = os.environ.get("CHANGE_IP_URL", "").strip().strip('"').strip("'")

def format_proxy(proxy_str):
    if not proxy_str: return None
    # –ï—Å–ª–∏ –ø—Ä–æ—Ç–æ–∫–æ–ª —É–∂–µ –µ—Å—Ç—å, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if proxy_str.startswith(('http://', 'https://', 'socks5://')):
        return proxy_str
    # –î–ª—è —Ç–≤–æ–µ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ login:pass@host:port –¥–æ–±–∞–≤–ª—è–µ–º http://
    return f"http://{proxy_str}"

PROXY_URL = format_proxy(RAW_PROXY)
if PROXY_URL:
    # –í—ã–≤–æ–¥–∏–º –≤ –ª–æ–≥ —á–∞—Å—Ç—å –ø—Ä–æ–∫—Å–∏ –±–µ–∑ –ø–∞—Ä–æ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    safe_log = PROXY_URL.split('@')[-1] if '@' in PROXY_URL else PROXY_URL
    logger.info(f"üåê –ü—Ä–æ–∫—Å–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {safe_log}")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–¥–µ—Ä–∂–µ–∫
PAGE_DELAY = (4.0, 7.0)
CONFIG_DELAY = (12.0, 20.0)

@dataclass
class PriceStat:
    model_name: str
    processor: str
    ram: int
    ssd: int
    median_price: int
    buyout_price: int
    samples_count: int
    updated_at: str

def rotate_ip():
    if CHANGE_IP_URL:
        try:
            logger.info("üîÑ –°–º–µ–Ω–∞ IP —á–µ—Ä–µ–∑ API...")
            requests.get(CHANGE_IP_URL, timeout=15, verify=False)
            time.sleep(12) # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ
        except Exception as e:
            logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–º–µ–Ω—ã IP: {e}")

def get_market_low(prices: list[int]) -> int:
    """–¢–≤–æ—è –º–µ—Ç–æ–¥–∏–∫–∞: 20-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –ø–æ—Å–ª–µ –æ—Ç—Å–µ—á–µ–Ω–∏—è 10% –º—É—Å–æ—Ä–∞"""
    if not prices: return 0
    prices = sorted(prices)
    n = len(prices)
    if n < 5: return int(statistics.median(prices))
    clean_prices = prices[int(n*0.1):] 
    idx = int(len(clean_prices) * 0.2) 
    return clean_prices[idx]

def parse_config(entry):
    url = entry['url']
    prices = []
    logger.info(f"üîé –ê–Ω–∞–ª–∏–∑: {entry['model_name']} {entry['ram']}/{entry['ssd']}...")
    
    proxies = {"http": PROXY_URL, "https": PROXY_URL} if PROXY_URL else None
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}

    for page in range(1, 3):
        try:
            time.sleep(random.uniform(*PAGE_DELAY))
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤ —Å—Å—ã–ª–∫–µ –Ω–µ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
            target_url = f"{url.strip()}&p={page}"
            resp = requests.get(target_url, headers=headers, proxies=proxies, timeout=25, verify=False)
            
            if resp.status_code == 429:
                logger.warning("‚ö†Ô∏è –õ–æ–≤—É—à–∫–∞ 429! –†–æ—Ç–∏—Ä—É–µ–º IP...")
                rotate_ip()
                continue

            if resp.status_code != 200: 
                logger.error(f"‚ùå –ö–æ–¥ {resp.status_code} –¥–ª—è {entry['model_name']}")
                break

            soup = BeautifulSoup(resp.text, 'lxml')
            items = soup.select('[data-marker="item"]')
            for item in items:
                p_tag = item.select_one('[itemprop="price"]')
                if p_tag:
                    try:
                        p = int(p_tag['content'])
                        if 20000 < p < 800000: prices.append(p)
                    except: continue
            if len(items) < 10: break
        except Exception as e:
            logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}")
            break
    
    if len(prices) < 5: return None
    
    market_low = get_market_low(prices)
    # –§–æ—Ä–º—É–ª–∞: —Ç–æ—á–∫–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –º–∏–Ω—É—Å 12–∫
    buyout = int((market_low - 12000) // 1000 * 1000)
    
    return PriceStat(
        model_name=entry['model_name'], processor=entry['processor'],
        ram=entry['ram'], ssd=entry['ssd'],
        median_price=market_low, buyout_price=buyout,
        samples_count=len(prices), updated_at=time.ctime()
    )

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch", default="1")
    parser.add_argument("--total-batches", type=int, default=3)
    args = parser.parse_args()

    if not URLS_FILE.exists():
        logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {URLS_FILE}")
        return

    with open(URLS_FILE, 'r', encoding='utf-8') as f:
        all_entries = json.load(f)['entries']

    batch_env = os.environ.get("BATCH", args.batch)
    if batch_env == "all":
        my_entries = all_entries
        logger.info(f"üì¶ –†–µ–∂–∏–º: –í–°–ï –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ({len(my_entries)} —à—Ç)")
    else:
        try:
            b_idx = int(batch_env)
            chunk = len(all_entries) // args.total_batches
            start = (b_idx - 1) * chunk
            end = b_idx * chunk if b_idx < args.total_batches else len(all_entries)
            my_entries = all_entries[start:end]
            logger.info(f"üì¶ –ë–∞—Ç—á {b_idx}/{args.total_batches} ({len(my_entries)} —à—Ç)")
        except:
            my_entries = all_entries
            logger.warning("‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –±–∞—Ç—á–∞, –±–µ—Ä–µ–º –≤—Å—ë")

    new_results = []
    for entry in my_entries:
        res = parse_config(entry)
        if res: new_results.append(asdict(res))
        time.sleep(random.uniform(*CONFIG_DELAY))

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä—É—é –±–∞–∑—É –¥–ª—è —Å–ª–∏—è–Ω–∏—è
    data = {"stats": []}
    if OUTPUT_FILE.exists():
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f: data = json.load(f)
        except: pass

    # –ú–µ—Ä–∂ (–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–º–µ–Ω—è—é—Ç —Å—Ç–∞—Ä—ã–µ)
    db = {(s['model_name'], s['ram'], s['ssd']): s for s in data['stats']}
    for s in new_results:
        db[(s['model_name'], s['ram'], s['ssd'])] = s

    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump({"updated_at": time.ctime(), "stats": list(db.values())}, f, ensure_ascii=False, indent=2)
    logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ. –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ: {len(new_results)}. –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(db)}")

if __name__ == "__main__":
    main()
