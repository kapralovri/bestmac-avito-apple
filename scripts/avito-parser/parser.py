#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä —Ü–µ–Ω MacBook —Å –ê–≤–∏—Ç–æ –¥–ª—è BestMac.ru

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É URL –∏–∑ avito-urls.json –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π.
–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ = –º–æ–¥–µ–ª—å + RAM + SSD + –≥–æ—Ç–æ–≤–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–∏—Å–∫ Avito —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏.

–§–æ—Ä–º–∞—Ç –º–æ–¥–µ–ª–∏: "MacBook Pro 14 (2021, M1 Pro)"

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  python parser.py

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
  pip install requests beautifulsoup4 lxml
"""

import json
import re
import time
import random
import statistics
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Optional

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install requests beautifulsoup4 lxml")
    exit(1)


# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
SCRIPT_DIR = Path(__file__).parent
URLS_FILE = SCRIPT_DIR / "../../public/data/avito-urls.json"
OUTPUT_FILE = SCRIPT_DIR / "../../public/data/avito-prices.json"


@dataclass
class PriceStat:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–Ω –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    model_name: str      # "MacBook Pro 14 (2021, M1 Pro)"
    processor: str       # "Apple M1", "Apple M1 Pro"
    ram: int             # GB
    ssd: int             # GB
    median_price: int
    min_price: int
    max_price: int
    buyout_price: int
    samples_count: int
    updated_at: str


# User-Agent –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]


def load_urls_config() -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    if not URLS_FILE.exists():
        print(f"‚ùå –§–∞–π–ª {URLS_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return {"entries": []}
    
    with open(URLS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_price(price_text: str) -> Optional[int]:
    """–ò–∑–≤–ª–µ—á—å —Ü–µ–Ω—É –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not price_text:
        return None
    
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä
    digits = re.sub(r'[^\d]', '', price_text)
    if not digits:
        return None
    
    price = int(digits)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ü–µ–Ω—ã –¥–ª—è MacBook
    if price < 20000 or price > 700000:
        return None
    
    return price


def parse_avito_page(url: str, page: int = 1) -> list[int]:
    """–°–ø–∞—Ä—Å–∏—Ç—å –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É Avito –∏ –≤–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ —Ü–µ–Ω"""
    prices = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫ URL
    page_url = url
    if page > 1:
        separator = '&' if '?' in url else '?'
        page_url = f"{url}{separator}p={page}"
    
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Cache-Control': 'no-cache',
    }
    
    max_retries = 3
    retry_delay = 5
    
    for attempt in range(max_retries):
        try:
            # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(random.uniform(2.0, 4.0))
            
            response = requests.get(page_url, headers=headers, timeout=30)
            
            # –ï—Å–ª–∏ 429 (Too Many Requests), –∂–¥–µ–º –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º
            if response.status_code == 429:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (attempt + 2)
                    print(f"    ‚è≥ 429 –æ—à–∏–±–∫–∞, –∂–¥–µ–º {wait_time} —Å–µ–∫...")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"    ‚ö†Ô∏è 429 –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                    return prices
            
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            items = soup.select('[data-marker="item"]')
            
            if not items:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
                items = soup.select('.iva-item-root')
            
            for item in items:
                try:
                    # –ò—â–µ–º —Ü–µ–Ω—É –≤ itemprop="price"
                    price_elem = item.select_one('[itemprop="price"]')
                    price = None
                    
                    if price_elem:
                        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º content –∞—Ç—Ä–∏–±—É—Ç
                        price_content = price_elem.get('content')
                        if price_content:
                            try:
                                price = int(float(price_content))
                            except ValueError:
                                pass
                        
                        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø–∞—Ä—Å–∏–º —Ç–µ–∫—Å—Ç
                        if not price:
                            price = extract_price(price_elem.get_text())
                    
                    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                    if not price:
                        alt_price = item.select_one('[data-marker="item-price"]')
                        if alt_price:
                            price = extract_price(alt_price.get_text())
                    
                    if price:
                        prices.append(price)
                        
                except Exception:
                    continue
            
            # –£—Å–ø–µ—à–Ω–æ - –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø–æ–≤—Ç–æ—Ä–æ–≤
            break
            
        except requests.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = retry_delay * (attempt + 1)
                print(f"    ‚è≥ –û—à–∏–±–∫–∞ —Å–µ—Ç–∏, –∂–¥–µ–º {wait_time} —Å–µ–∫...")
                time.sleep(wait_time)
            else:
                print(f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
    
    return prices


def parse_entry(entry: dict, pages_count: int = 3) -> Optional[PriceStat]:
    """–°–ø–∞—Ä—Å–∏—Ç—å –æ–¥–Ω—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —Ç–∞–±–ª–∏—Ü—ã"""
    model_name = entry.get("model_name", "")
    processor = entry.get("processor", "")
    ram = entry.get("ram", 0)
    ssd = entry.get("ssd", 0)
    url = entry.get("url", "")
    
    if not url:
        print(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {model_name} - –Ω–µ—Ç URL")
        return None
    
    print(f"\nüîç {model_name} | {processor} | {ram}GB RAM | {ssd}GB SSD")
    
    all_prices = []
    
    # –ü–∞—Ä—Å–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    for page in range(1, pages_count + 1):
        print(f"    üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}...", end=" ", flush=True)
        page_prices = parse_avito_page(url, page)
        print(f"–Ω–∞–π–¥–µ–Ω–æ {len(page_prices)} —Ü–µ–Ω")
        all_prices.extend(page_prices)
        
        # –ï—Å–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –º–∞–ª–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º
        if len(page_prices) < 10:
            break
    
    if len(all_prices) < 2:
        print(f"  ‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(all_prices)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π)")
        return None
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –≤—ã–±—Ä–æ—Å—ã –ø–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è–º (P10-P90)
    all_prices.sort()
    n = len(all_prices)
    
    if n >= 5:
        p10_idx = int(n * 0.10)
        p90_idx = int(n * 0.90)
        filtered_prices = all_prices[p10_idx:p90_idx + 1]
    else:
        filtered_prices = all_prices
    
    if not filtered_prices:
        filtered_prices = all_prices
    
    # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    median_price = int(statistics.median(filtered_prices))
    min_price = min(filtered_prices)
    max_price = max(filtered_prices)
    buyout_price = int(median_price * 0.90)  # 90% –æ—Ç –º–µ–¥–∏–∞–Ω—ã
    
    print(f"  ‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(all_prices)} —Ü–µ–Ω | –ú–µ–¥–∏–∞–Ω–∞: {median_price:,} ‚ÇΩ | –í—ã–∫—É–ø: {buyout_price:,} ‚ÇΩ")
    
    return PriceStat(
        model_name=model_name,
        processor=processor,
        ram=ram,
        ssd=ssd,
        median_price=median_price,
        min_price=min_price,
        max_price=max_price,
        buyout_price=buyout_price,
        samples_count=len(all_prices),
        updated_at=datetime.now().isoformat()
    )


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
    print("=" * 70)
    print("üöÄ –ü–∞—Ä—Å–µ—Ä —Ü–µ–Ω MacBook —Å –ê–≤–∏—Ç–æ (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∞–±–ª–∏—Ü—ã URL)")
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É URL
    config = load_urls_config()
    entries = config.get("entries", [])
    
    if not entries:
        print("\n‚ùå –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞!")
        print(f"   –î–æ–±–∞–≤—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª: {URLS_FILE}")
        
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "generated_at": datetime.now().isoformat(),
            "total_listings": 0,
            "models": [],
            "stats": []
        }
        
        OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        return
    
    print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ {len(entries)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
    
    # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    stats = []
    total_listings = 0
    
    for i, entry in enumerate(entries, 1):
        print(f"\n[{i}/{len(entries)}]", end="")
        stat = parse_entry(entry, pages_count=3)
        if stat:
            stats.append(asdict(stat))
            total_listings += stat.samples_count
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
    unique_models = sorted(set(s["model_name"] for s in stats))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = {
        "generated_at": datetime.now().isoformat(),
        "total_listings": total_listings,
        "models": unique_models,
        "stats": stats
    }
    
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 70)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {len(stats)}/{len(entries)}")
    print(f"   üìà –í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {total_listings:,}")
    print(f"   üè∑Ô∏è –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {len(unique_models)}")
    print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {OUTPUT_FILE}")
    print("=" * 70)


if __name__ == "__main__":
    main()
