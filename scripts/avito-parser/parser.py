#!/usr/bin/env python3
import json
import os
import re
import time
import random
import statistics
import argparse
import logging
from pathlib import Path
from dataclasses import dataclass, asdict
from urllib.parse import urljoin

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("‚ùå –û—à–∏–±–∫–∞: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫–æ–º–∞–Ω–¥–æ–π: pip install requests beautifulsoup4 lxml")
    exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–æ–≤
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("Parser")

# –ü—É—Ç–∏
SCRIPT_DIR = Path(__file__).parent
URLS_FILE = SCRIPT_DIR / "../../public/data/avito-urls.json"
OUTPUT_FILE = SCRIPT_DIR / "../../public/data/avito-prices.json"

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
PROXY_URL = os.environ.get("PROXY_URL")
CHANGE_IP_URL = os.environ.get("CHANGE_IP_URL")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–¥–µ—Ä–∂–µ–∫
PAGE_DELAY = (4.0, 8.0)
CONFIG_DELAY = (15.0, 25.0)

@dataclass
class PriceStat:
    model_name: str
    processor: str
    ram: int
    ssd: int
    median_price: int
    buyout_price: int
    samples_count: int
    updated_at: str

def rotate_ip():
    """–í—ã–∑–æ–≤ API –¥–ª—è —Å–º–µ–Ω—ã IP –º–æ–±–∏–ª—å–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏"""
    if CHANGE_IP_URL:
        try:
            logger.info("üîÑ –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–º–µ–Ω—É IP...")
            requests.get(CHANGE_IP_URL, timeout=15, verify=False)
            time.sleep(15) # –ñ–¥–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
        except Exception as e:
            logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–º–µ–Ω—ã IP: {e}")

def get_market_low(prices: list[int]) -> int:
    """–ê–ª–≥–æ—Ä–∏—Ç–º '–ù–∏–∂–Ω–µ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏': –æ—Ç—Å–µ–∫–∞–µ–º 10% –º—É—Å–æ—Ä–∞ –∏ –±–µ—Ä–µ–º 20-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å"""
    if not prices: return 0
    prices = sorted(prices)
    n = len(prices)
    if n < 5: return int(statistics.median(prices))
    
    # –û—Ç—Å–µ–∫–∞–µ–º –Ω–∏–∂–Ω–∏–µ 10% (–∑–∞–ø—á–∞—Å—Ç–∏, —Å–∫–∞–º)
    clean_prices = prices[int(n*0.1):]
    # –ë–µ—Ä–µ–º —Ç–æ—á–∫—É –Ω–∞ —É—Ä–æ–≤–Ω–µ 20% (–Ω–∞—á–∞–ª–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
    idx = int(len(clean_prices) * 0.2)
    return clean_prices[idx]

def parse_config(entry):
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)"""
    url = entry['url']
    prices = []
    logger.info(f"üîé –ê–Ω–∞–ª–∏–∑: {entry['model_name']} {entry['ram']}/{entry['ssd']}...")
    
    proxies = {"http": PROXY_URL, "https": PROXY_URL} if PROXY_URL else None
    headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"}

    for page in range(1, 3):
        try:
            time.sleep(random.uniform(*PAGE_DELAY))
            target_url = f"{url}&p={page}"
            resp = requests.get(target_url, headers=headers, proxies=proxies, timeout=25)
            
            if resp.status_code == 429:
                logger.warning("‚ö†Ô∏è –ö–æ–¥ 429! –ü—Ä–æ–±—É–µ–º —Å–º–µ–Ω–∏—Ç—å IP...")
                rotate_ip()
                continue

            if resp.status_code != 200: 
                logger.error(f"‚ùå –ö–æ–¥ {resp.status_code} –¥–ª—è {target_url}")
                break

            soup = BeautifulSoup(resp.text, 'lxml')
            items = soup.select('[data-marker="item"]')
            
            for item in items:
                p_tag = item.select_one('[itemprop="price"]')
                if p_tag:
                    try:
                        p = int(p_tag['content'])
                        if 20000 < p < 800000: prices.append(p)
                    except: continue
            
            if len(items) < 10: break # –°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª—É–ø—É—Å—Ç–∞—è
            
        except Exception as e:
            logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}")
            break
    
    if len(prices) < 5:
        logger.warning(f"üìâ –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {entry['model_name']} ({len(prices)} —Ü–µ–Ω)")
        return None
    
    market_low = get_market_low(prices)
    # –§–æ—Ä–º—É–ª–∞: –ù–∏–∑ —Ä—ã–Ω–∫–∞ - 12 000 —Ä—É–±.
    buyout = int((market_low - 12000) // 1000 * 1000)
    
    return PriceStat(
        model_name=entry['model_name'],
        processor=entry['processor'],
        ram=entry['ram'],
        ssd=entry['ssd'],
        median_price=market_low,
        buyout_price=buyout,
        samples_count=len(prices),
        updated_at=time.ctime()
    )

def main():
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch", default="1")
    parser.add_argument("--total-batches", type=int, default=3)
    args = parser.parse_args()

    if not URLS_FILE.exists():
        logger.error(f"‚ùå –§–∞–π–ª URL –Ω–µ –Ω–∞–π–¥–µ–Ω: {URLS_FILE}")
        return

    with open(URLS_FILE, 'r', encoding='utf-8') as f:
        all_entries = json.load(f)['entries']

    # –õ–æ–≥–∏–∫–∞ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –±–∞—Ç—á–∏
    if args.batch == "all":
        logger.info("üì¶ –†–µ–∂–∏–º: –ü–∞—Ä—Å–∏–º –í–°–ï –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        my_entries = all_entries
    else:
        try:
            b_idx = int(args.batch)
            chunk = len(all_entries) // args.total_batches
            start = (b_idx - 1) * chunk
            end = b_idx * chunk if b_idx < args.total_batches else len(all_entries)
            my_entries = all_entries[start:end]
            logger.info(f"üì¶ –†–µ–∂–∏–º: –ë–∞—Ç—á {b_idx} –∏–∑ {args.total_batches} ({len(my_entries)} –∑–∞–ø–∏—Å–µ–π)")
        except:
            logger.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –±–∞—Ç—á–∞")
            return

    new_results = []
    for entry in my_entries:
        stat = parse_config(entry)
        if stat:
            new_results.append(asdict(stat))
        time.sleep(random.uniform(*CONFIG_DELAY))

    # –°–ª–∏—è–Ω–∏–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∞–∑–æ–π
    existing_data = {"stats": []}
    if OUTPUT_FILE.exists():
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except: pass

    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–ø–æ –∫–ª—é—á—É –º–æ–¥–µ–ª—å+—Ä–∞–º+—Å—Å–¥)
    db = {(s['model_name'], s['ram'], s['ssd']): s for s in existing_data['stats']}
    for s in new_results:
        db[(s['model_name'], s['ram'], s['ssd'])] = s

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        output = {
            "updated_at": time.ctime(),
            "stats": list(db.values())
        }
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ. –ë–∞–∑–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç {len(db)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π.")

if __name__ == "__main__":
    main()
