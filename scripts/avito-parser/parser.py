#!/usr/bin/env python3
import json
import os
import re
import time
import random
import statistics
from pathlib import Path
from dataclasses import dataclass, asdict
import requests
from bs4 import BeautifulSoup

# –ü—É—Ç–∏
SCRIPT_DIR = Path(__file__).parent
URLS_FILE = SCRIPT_DIR / "../../public/data/avito-urls.json"
OUTPUT_FILE = SCRIPT_DIR / "../../public/data/avito-prices.json"

# –£—Å–∫–æ—Ä–µ–Ω–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏
PAGE_DELAY = (3.0, 6.0)
CONFIG_DELAY = (10.0, 20.0)

@dataclass
class PriceStat:
    model_name: str
    processor: str
    ram: int
    ssd: int
    median_price: int
    buyout_price: int
    samples_count: int
    updated_at: str

def get_market_low(prices: list[int]) -> int:
    """–¢–≤–æ—è –º–µ—Ç–æ–¥–∏–∫–∞: –ù–∞—Ö–æ–¥–∏–º –Ω–∞—á–∞–ª–æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –≤ –¥–µ—à–µ–≤–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ"""
    if not prices: return 0
    prices = sorted(prices)
    n = len(prices)
    if n < 5: return int(statistics.median(prices))
    
    # 1. –û—Ç—Å–µ–∫–∞–µ–º –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –¥–µ—à–µ–≤—ã–µ (–Ω–∏–∂–Ω–∏–µ 10% - –∑–∞–ø—á–∞—Å—Ç–∏/—Å–∫–∞–º)
    clean_prices = prices[int(n*0.1):]
    
    # 2. –ë–µ—Ä–µ–º —Ü–µ–Ω—É –Ω–∞ —É—Ä–æ–≤–Ω–µ 20% –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ (—ç—Ç–æ –∏ –µ—Å—Ç—å –Ω–∞—á–∞–ª–æ '–ø–æ–ª–∫–∏' —Ü–µ–Ω)
    idx = int(len(clean_prices) * 0.2)
    return clean_prices[idx]

def parse_config(entry):
    url = entry['url']
    prices = []
    print(f"üîé –ê–Ω–∞–ª–∏–∑: {entry['model_name']} {entry['ram']}/{entry['ssd']}...")
    
    for page in range(1, 3): # 2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä—ã–Ω–∫–∞
        try:
            time.sleep(random.uniform(*PAGE_DELAY))
            resp = requests.get(url + f"&p={page}", timeout=20)
            if resp.status_code != 200: break
            soup = BeautifulSoup(resp.text, 'lxml')
            items = soup.select('[data-marker="item"]')
            for item in items:
                p_tag = item.select_one('[itemprop="price"]')
                if p_tag:
                    p = int(p_tag['content'])
                    if 20000 < p < 800000: prices.append(p)
            if len(items) < 15: break
        except: break
    
    if len(prices) < 5: return None
    
    market_low = get_market_low(prices)
    # –¢–≤–æ—è —Ñ–æ—Ä–º—É–ª–∞: —Ç–æ—á–∫–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –º–∏–Ω—É—Å 12–∫
    buyout = int((market_low - 12000) // 1000 * 1000)
    
    return PriceStat(
        model_name=entry['model_name'], processor=entry['processor'],
        ram=entry['ram'], ssd=entry['ssd'],
        median_price=market_low, buyout_price=buyout,
        samples_count=len(prices), updated_at=time.ctime()
    )

def main():
    if not URLS_FILE.exists(): return
    with open(URLS_FILE, 'r') as f: entries = json.load(f)['entries']
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–∞—Ç—á–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    b_idx = int(os.environ.get("BATCH", 1))
    t_batches = int(os.environ.get("TOTAL_BATCHES", 1))
    chunk = len(entries) // t_batches
    my_entries = entries[(b_idx-1)*chunk : b_idx*chunk]
    
    new_stats = []
    for entry in my_entries:
        res = parse_config(entry)
        if res: new_stats.append(asdict(res))
        time.sleep(random.uniform(*CONFIG_DELAY))
        
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (—Å–ª–∏—è–Ω–∏–µ —Å —Ç–µ–∫—É—â–µ–π –±–∞–∑–æ–π)
    data = {"stats": []}
    if OUTPUT_FILE.exists():
        try:
            with open(OUTPUT_FILE, 'r') as f: data = json.load(f)
        except: pass
        
    # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å–∏
    current = { (s['model_name'], s['ram'], s['ssd']): s for s in data['stats'] }
    for s in new_stats:
        current[(s['model_name'], s['ram'], s['ssd'])] = s
        
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump({"updated_at": time.ctime(), "stats": list(current.values())}, f, ensure_ascii=False, indent=2)
    print("‚úÖ –ë–∞–∑–∞ —Ü–µ–Ω –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")

if __name__ == "__main__":
    main()
