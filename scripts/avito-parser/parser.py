#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä —Ü–µ–Ω MacBook —Å –ê–≤–∏—Ç–æ –¥–ª—è BestMac.ru
–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –º–æ–¥–µ–ª—è–º.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  python parser.py --output ../public/data/avito-prices.json

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
  pip install requests beautifulsoup4 lxml
"""

import argparse
import json
import re
import time
import random
from datetime import datetime, timedelta
from typing import Optional
from dataclasses import dataclass, asdict
from pathlib import Path

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install requests beautifulsoup4 lxml")
    exit(1)


@dataclass
class AvitoListing:
    """–û–±—ä—è–≤–ª–µ–Ω–∏–µ —Å –ê–≤–∏—Ç–æ"""
    title: str
    price: int
    url: str
    region: str
    published_date: Optional[str] = None


@dataclass
class MacModel:
    """–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å MacBook"""
    type: str  # "Air" | "Pro"
    year: Optional[int] = None
    cpu: Optional[str] = None  # "M1" | "M2" | "M3" | "M4" | "Intel"
    ram: Optional[int] = None  # GB
    ssd: Optional[int] = None  # GB
    screen: Optional[float] = None  # 13.3, 14, 15, 16


@dataclass
class PriceStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–Ω –¥–ª—è –º–æ–¥–µ–ª–∏"""
    model: str
    cpu: str
    year: int
    ram: int
    ssd: int
    region: str
    median_price: int
    min_price: int
    max_price: int
    buyout_price: int
    samples_count: int
    updated_at: str


# –†–µ–≥–∏–æ–Ω—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
REGIONS = {
    "moskva": "–ú–æ—Å–∫–≤–∞",
    "moskovskaya_oblast": "–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
    "sankt-peterburg": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
}

# –ú–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
SEARCH_QUERIES = [
    "macbook air m1",
    "macbook air m2", 
    "macbook air m3",
    "macbook pro m1",
    "macbook pro m2",
    "macbook pro m3",
    "macbook pro m4",
    "macbook pro 14",
    "macbook pro 16",
]

# User-Agent –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]


def parse_price(text: str) -> Optional[int]:
    """–ò–∑–≤–ª–µ—á—å —Ü–µ–Ω—É –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return None
    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä
    digits = re.sub(r'[^\d]', '', text)
    if digits:
        price = int(digits)
        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ü–µ–Ω—ã
        if 10000 <= price <= 500000:
            return price
    return None


def parse_model(title: str) -> Optional[MacModel]:
    """–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –º–æ–¥–µ–ª—å MacBook –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
    title_lower = title.lower()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø
    if 'air' in title_lower:
        mac_type = 'Air'
    elif 'pro' in title_lower:
        mac_type = 'Pro'
    else:
        return None
    
    model = MacModel(type=mac_type)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    cpu_patterns = [
        (r'm4\s*(pro|max)?', 'M4'),
        (r'm3\s*(pro|max)?', 'M3'),
        (r'm2\s*(pro|max)?', 'M2'),
        (r'm1\s*(pro|max)?', 'M1'),
        (r'intel|i[579]', 'Intel'),
    ]
    for pattern, cpu in cpu_patterns:
        if re.search(pattern, title_lower):
            model.cpu = cpu
            break
    
    # –ì–æ–¥
    year_match = re.search(r'20(1[89]|2[0-5])', title)
    if year_match:
        model.year = int(year_match.group())
    
    # RAM
    ram_match = re.search(r'(\d{1,2})\s*(gb|–≥–±)\s*(ram|–æ–∑—É|–ø–∞–º—è—Ç—å)?', title_lower)
    if ram_match:
        ram = int(ram_match.group(1))
        if ram in [8, 16, 18, 24, 32, 36, 48, 64, 96, 128]:
            model.ram = ram
    
    # SSD
    ssd_patterns = [
        (r'(\d{3,4})\s*(gb|–≥–±)\s*(ssd)?', lambda m: int(m.group(1))),
        (r'(\d)\s*(tb|—Ç–±)', lambda m: int(m.group(1)) * 1024),
    ]
    for pattern, extractor in ssd_patterns:
        ssd_match = re.search(pattern, title_lower)
        if ssd_match:
            ssd = extractor(ssd_match)
            if ssd in [256, 512, 1024, 2048, 4096, 8192]:
                model.ssd = ssd
                break
    
    # –†–∞–∑–º–µ—Ä —ç–∫—Ä–∞–Ω–∞
    screen_match = re.search(r'(13|14|15|16)["\']?[\s\-]?(–¥—é–π–º)?', title_lower)
    if screen_match:
        model.screen = float(screen_match.group(1))
    
    return model


def fetch_avito_page(query: str, region: str, page: int = 1) -> list[AvitoListing]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å –ê–≤–∏—Ç–æ"""
    listings = []
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º URL
    base_url = f"https://www.avito.ru/{region}/noutbuki"
    params = {
        'q': query,
        'p': page,
    }
    
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        'Connection': 'keep-alive',
    }
    
    try:
        response = requests.get(base_url, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'lxml')
        
        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        items = soup.select('[data-marker="item"]')
        
        for item in items:
            try:
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                title_elem = item.select_one('[itemprop="name"]')
                title = title_elem.get_text(strip=True) if title_elem else None
                
                # –¶–µ–Ω–∞
                price_elem = item.select_one('[itemprop="price"]')
                price = None
                if price_elem:
                    price_content = price_elem.get('content')
                    if price_content:
                        price = int(price_content)
                    else:
                        price = parse_price(price_elem.get_text())
                
                # –°—Å—ã–ª–∫–∞
                link_elem = item.select_one('a[itemprop="url"]')
                url = f"https://www.avito.ru{link_elem['href']}" if link_elem else None
                
                if title and price and url:
                    listings.append(AvitoListing(
                        title=title,
                        price=price,
                        url=url,
                        region=REGIONS.get(region, region),
                    ))
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
                continue
                
    except requests.RequestException as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {query} –≤ {region}: {e}")
    
    return listings


def calculate_percentiles(prices: list[int], lower: float = 10, upper: float = 90) -> list[int]:
    """–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –≤—ã–±—Ä–æ—Å—ã –ø–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è–º"""
    if len(prices) < 5:
        return prices
    
    sorted_prices = sorted(prices)
    n = len(sorted_prices)
    lower_idx = int(n * lower / 100)
    upper_idx = int(n * upper / 100)
    
    return sorted_prices[lower_idx:upper_idx + 1]


def aggregate_prices(listings: list[AvitoListing]) -> list[PriceStats]:
    """–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–Ω—ã –ø–æ –º–æ–¥–µ–ª—è–º"""
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
    groups: dict[tuple, list[int]] = {}
    
    for listing in listings:
        model = parse_model(listing.title)
        if not model or not model.cpu:
            continue
        
        # –ö–ª—é—á –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        key = (
            f"MacBook {model.type}",
            model.cpu,
            model.year or 2023,  # –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –≥–æ–¥
            model.ram or 8,  # –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π RAM
            model.ssd or 256,  # –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π SSD
            listing.region,
        )
        
        if key not in groups:
            groups[key] = []
        groups[key].append(listing.price)
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = []
    for key, prices in groups.items():
        if len(prices) < 3:  # –º–∏–Ω–∏–º—É–º 3 –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            continue
        
        # –û—á–∏—â–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
        clean_prices = calculate_percentiles(prices)
        if not clean_prices:
            continue
        
        # –ú–µ–¥–∏–∞–Ω–∞
        sorted_prices = sorted(clean_prices)
        n = len(sorted_prices)
        median = sorted_prices[n // 2] if n % 2 else (sorted_prices[n // 2 - 1] + sorted_prices[n // 2]) // 2
        
        # –¶–µ–Ω–∞ –≤—ã–∫—É–ø–∞ (90% –æ—Ç –º–µ–¥–∏–∞–Ω—ã)
        buyout = int(median * 0.90)
        
        stats.append(PriceStats(
            model=key[0],
            cpu=key[1],
            year=key[2],
            ram=key[3],
            ssd=key[4],
            region=key[5],
            median_price=median,
            min_price=min(clean_prices),
            max_price=max(clean_prices),
            buyout_price=buyout,
            samples_count=len(prices),
            updated_at=datetime.now().isoformat(),
        ))
    
    return stats


def run_parser(output_path: str, max_pages: int = 3):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞—Ä—Å–µ—Ä"""
    print(f"–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –ê–≤–∏—Ç–æ...")
    print(f"–†–µ–≥–∏–æ–Ω—ã: {list(REGIONS.values())}")
    print(f"–ó–∞–ø—Ä–æ—Å—ã: {SEARCH_QUERIES}")
    
    all_listings = []
    
    for region_key, region_name in REGIONS.items():
        print(f"\nüìç –†–µ–≥–∏–æ–Ω: {region_name}")
        
        for query in SEARCH_QUERIES:
            print(f"  üîç {query}...", end=" ")
            
            query_listings = []
            for page in range(1, max_pages + 1):
                listings = fetch_avito_page(query, region_key, page)
                query_listings.extend(listings)
                
                if not listings:
                    break
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(random.uniform(2, 5))
            
            print(f"–Ω–∞–π–¥–µ–Ω–æ {len(query_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            all_listings.extend(query_listings)
    
    print(f"\nüìä –í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(all_listings)}")
    
    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º
    stats = aggregate_prices(all_listings)
    print(f"üìà –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {len(stats)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    output = Path(output_path)
    output.parent.mkdir(parents=True, exist_ok=True)
    
    result = {
        "generated_at": datetime.now().isoformat(),
        "total_listings": len(all_listings),
        "stats": [asdict(s) for s in stats],
    }
    
    with open(output, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_path}")
    return result


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ü–∞—Ä—Å–µ—Ä —Ü–µ–Ω MacBook —Å –ê–≤–∏—Ç–æ")
    parser.add_argument(
        "--output", 
        default="public/data/avito-prices.json",
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON"
    )
    parser.add_argument(
        "--pages",
        type=int,
        default=3,
        help="–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"
    )
    
    args = parser.parse_args()
    run_parser(args.output, args.pages)
