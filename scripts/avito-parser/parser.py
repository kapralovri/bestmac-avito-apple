#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä —Ü–µ–Ω MacBook —Å –ê–≤–∏—Ç–æ –¥–ª—è BestMac.ru

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É URL –∏–∑ avito-urls.json –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π.
–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ = –º–æ–¥–µ–ª—å + RAM + SSD + –≥–æ—Ç–æ–≤–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–∏—Å–∫ Avito —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏.

–§–æ—Ä–º–∞—Ç –º–æ–¥–µ–ª–∏: "MacBook Pro 14 (2021, M1 Pro)"

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  python parser.py [pages]  # pages = –∫–æ–ª-–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2)

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
  pip install requests beautifulsoup4 lxml
"""

import json
import os
import re
import time
import random
import statistics
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Optional

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install requests beautifulsoup4 lxml")
    exit(1)


# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
SCRIPT_DIR = Path(__file__).parent
URLS_FILE = SCRIPT_DIR / "../../public/data/avito-urls.json"
OUTPUT_FILE = SCRIPT_DIR / "../../public/data/avito-prices.json"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è –æ–±—Ö–æ–¥–∞ rate limiting
# –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü (—Å–µ–∫—É–Ω–¥—ã)
PAGE_DELAY_MIN = 8.0
PAGE_DELAY_MAX = 15.0

# –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏ (—Å–µ–∫—É–Ω–¥—ã) - –≤–∞–∂–Ω–æ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è 429!
CONFIG_DELAY_MIN = 30.0
CONFIG_DELAY_MAX = 60.0

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (2 –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è ~100 –æ–±—ä—è–≤–ª–µ–Ω–∏–π)
DEFAULT_PAGES = 2


@dataclass
class PriceStat:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–Ω –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    model_name: str      # "MacBook Pro 14 (2021, M1 Pro)"
    processor: str       # "Apple M1", "Apple M1 Pro"
    ram: int             # GB
    ssd: int             # GB
    median_price: int
    min_price: int
    max_price: int
    buyout_price: int
    samples_count: int
    updated_at: str


# User-Agent –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ (—Ä–∞–∑–Ω—ã–µ –±—Ä–∞—É–∑–µ—Ä—ã)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
]


AVITO_HOME_URL = "https://www.avito.ru/"

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É —Å–µ—Å—Å–∏—é –Ω–∞ –≤–µ—Å—å –∑–∞–ø—É—Å–∫ (cookies + keep-alive)
SESSION = requests.Session()
SESSION.headers.update({
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
    "Cache-Control": "no-cache",
    "Upgrade-Insecure-Requests": "1",
})


def warm_up_avito() -> bool:
    """–ü—Ä–æ–≥—Ä–µ—Ç—å —Å–µ—Å—Å–∏—é: –ø–æ–ª—É—á–∏—Ç—å –±–∞–∑–æ–≤—ã–µ cookies —Å –≥–ª–∞–≤–Ω–æ–π.

    –ï—Å–ª–∏ GitHub Actions IP –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω, —á–∞—Å—Ç–æ 429 –ø—Ä–∏—Ö–æ–¥–∏—Ç —É–∂–µ —Ç—É—Ç.
    """
    try:
        headers = {
            "User-Agent": random.choice(USER_AGENTS),
            "Referer": AVITO_HOME_URL,
        }
        resp = SESSION.get(AVITO_HOME_URL, headers=headers, timeout=30)
        if resp.status_code == 429:
            print("\n‚ùå Avito –≤–µ—Ä–Ω—É–ª 429 –¥–∞–∂–µ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ ‚Äî –ø–æ—Ö–æ–∂–µ, IP —Å—Ä–µ–¥—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω.")
            return False
        return True
    except requests.RequestException as e:
        print(f"\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≥—Ä–µ—Ç—å —Å–µ—Å—Å–∏—é Avito: {e}")
        # –ù–µ –±–ª–æ–∫–∏—Ä—É–µ–º –∑–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑-–∑–∞ —Å–µ—Ç–µ–≤–æ–π –æ—à–∏–±–∫–∏
        return True


def load_urls_config() -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    if not URLS_FILE.exists():
        print(f"‚ùå –§–∞–π–ª {URLS_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return {"entries": []}
    
    with open(URLS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_price(price_text: str) -> Optional[int]:
    """–ò–∑–≤–ª–µ—á—å —Ü–µ–Ω—É –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not price_text:
        return None
    
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä
    digits = re.sub(r'[^\d]', '', price_text)
    if not digits:
        return None
    
    price = int(digits)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ü–µ–Ω—ã –¥–ª—è MacBook
    if price < 20000 or price > 700000:
        return None
    
    return price


def parse_avito_page(url: str, page: int = 1) -> list[int]:
    """–°–ø–∞—Ä—Å–∏—Ç—å –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É Avito –∏ –≤–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ —Ü–µ–Ω"""
    prices = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫ URL
    page_url = url
    if page > 1:
        separator = '&' if '?' in url else '?'
        page_url = f"{url}{separator}p={page}"
    
    max_retries = 3
    base_retry_delay = 15  # –±–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ 429
    retry_delay_cap = 90   # –≤–µ—Ä—Ö–Ω–∏–π –ø—Ä–µ–¥–µ–ª –æ–∂–∏–¥–∞–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ "–≤–∏—Å–µ—Ç—å" –¥–µ—Å—è—Ç–∫–∏ –º–∏–Ω—É—Ç
    
    for attempt in range(max_retries):
        try:
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º (–≤–∞–∂–Ω–æ!)
            delay = random.uniform(PAGE_DELAY_MIN, PAGE_DELAY_MAX)
            print(f"‚è≥ –ñ–¥—ë–º {delay:.1f}—Å...", end=" ", flush=True)
            time.sleep(delay)

            # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç—Ä–æ–∏–º –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å/–ø–æ–≤—Ç–æ—Ä (UA —Ä–æ—Ç—É–µ–º)
            headers = {
                "User-Agent": random.choice(USER_AGENTS),
                "Referer": AVITO_HOME_URL,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
                "Cache-Control": "no-cache",
                "Upgrade-Insecure-Requests": "1",
            }

            response = SESSION.get(page_url, headers=headers, timeout=30)
            
            # –ï—Å–ª–∏ 429 (Too Many Requests), –∂–¥–µ–º –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º
            if response.status_code == 429:
                if attempt < max_retries - 1:
                    retry_after = (response.headers.get("Retry-After") or "").strip()
                    if retry_after.isdigit():
                        wait_time = float(int(retry_after))
                    else:
                        wait_time = min(
                            retry_delay_cap,
                            base_retry_delay * (2 ** attempt) + random.uniform(5, 15),
                        )
                    print(f"\n    ‚ö†Ô∏è 429 –æ—à–∏–±–∫–∞, –∂–¥—ë–º {wait_time:.0f}—Å (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})...")
                    time.sleep(wait_time)
                    continue

                print(f"\n    ‚ùå 429 –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return prices

            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            items = soup.select('[data-marker="item"]')
            
            if not items:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
                items = soup.select('.iva-item-root')
            
            for item in items:
                try:
                    # –ò—â–µ–º —Ü–µ–Ω—É –≤ itemprop="price"
                    price_elem = item.select_one('[itemprop="price"]')
                    price = None
                    
                    if price_elem:
                        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º content –∞—Ç—Ä–∏–±—É—Ç
                        price_content = price_elem.get('content')
                        if price_content:
                            try:
                                price = int(float(price_content))
                            except ValueError:
                                pass
                        
                        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø–∞—Ä—Å–∏–º —Ç–µ–∫—Å—Ç
                        if not price:
                            price = extract_price(price_elem.get_text())
                    
                    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                    if not price:
                        alt_price = item.select_one('[data-marker="item-price"]')
                        if alt_price:
                            price = extract_price(alt_price.get_text())
                    
                    if price:
                        prices.append(price)
                        
                except Exception:
                    continue
            
            # –£—Å–ø–µ—à–Ω–æ - –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø–æ–≤—Ç–æ—Ä–æ–≤
            break
            
        except requests.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = base_retry_delay * (attempt + 1)
                print(f"\n    ‚è≥ –û—à–∏–±–∫–∞ —Å–µ—Ç–∏, –∂–¥—ë–º {wait_time}—Å...")
                time.sleep(wait_time)
            else:
                print(f"\n    ‚ùå –û—à–∏–±–∫–∞: {e}")
    
    return prices


def parse_entry(entry: dict, pages_count: int = DEFAULT_PAGES) -> Optional[PriceStat]:
    """–°–ø–∞—Ä—Å–∏—Ç—å –æ–¥–Ω—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —Ç–∞–±–ª–∏—Ü—ã"""
    model_name = entry.get("model_name", "")
    processor = entry.get("processor", "")
    ram = entry.get("ram", 0)
    ssd = entry.get("ssd", 0)
    url = entry.get("url", "")
    
    if not url:
        print(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {model_name} - –Ω–µ—Ç URL")
        return None
    
    print(f"\n{'='*60}")
    print(f"üîç {model_name} | {processor} | {ram}GB RAM | {ssd}GB SSD")
    print(f"{'='*60}")
    
    all_prices = []
    
    # –ü–∞—Ä—Å–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    for page in range(1, pages_count + 1):
        print(f"  üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}/{pages_count}:", end=" ", flush=True)
        page_prices = parse_avito_page(url, page)
        print(f"‚úÖ {len(page_prices)} —Ü–µ–Ω")
        all_prices.extend(page_prices)
        
        # –ï—Å–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –º–∞–ª–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º
        if len(page_prices) < 10:
            print(f"  ‚ÑπÔ∏è –ú–∞–ª–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º")
            break
    
    if len(all_prices) < 3:
        print(f"  ‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(all_prices)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π)")
        return None
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –≤—ã–±—Ä–æ—Å—ã –ø–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è–º (P10-P90)
    all_prices.sort()
    n = len(all_prices)
    
    if n >= 5:
        p10_idx = int(n * 0.10)
        p90_idx = int(n * 0.90)
        filtered_prices = all_prices[p10_idx:p90_idx + 1]
    else:
        filtered_prices = all_prices
    
    if not filtered_prices:
        filtered_prices = all_prices
    
    # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    median_price = int(statistics.median(filtered_prices))
    min_price = min(filtered_prices)
    max_price = max(filtered_prices)
    buyout_price = int(median_price * 0.90)  # 90% –æ—Ç –º–µ–¥–∏–∞–Ω—ã
    
    print(f"\n  üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"     ‚Ä¢ –°–æ–±—Ä–∞–Ω–æ: {len(all_prices)} —Ü–µ–Ω (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_prices)})")
    print(f"     ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω: {min_price:,} - {max_price:,} ‚ÇΩ")
    print(f"     ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {median_price:,} ‚ÇΩ")
    print(f"     ‚Ä¢ –¶–µ–Ω–∞ –≤—ã–∫—É–ø–∞: {buyout_price:,} ‚ÇΩ")
    
    return PriceStat(
        model_name=model_name,
        processor=processor,
        ram=ram,
        ssd=ssd,
        median_price=median_price,
        min_price=min_price,
        max_price=max_price,
        buyout_price=buyout_price,
        samples_count=len(all_prices),
        updated_at=datetime.now().isoformat()
    )


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
    import sys
    pages_count = DEFAULT_PAGES
    
    # –ò–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if len(sys.argv) > 1:
        try:
            pages_count = int(sys.argv[1])
        except ValueError:
            pass
    
    # –ò–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è (–¥–ª—è GitHub Actions)
    env_pages = os.environ.get('PAGES')
    if env_pages:
        try:
            pages_count = int(env_pages)
        except ValueError:
            pass
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º –¥–∏–∞–ø–∞–∑–æ–Ω–æ–º
    pages_count = max(1, min(pages_count, 5))
    
    print("=" * 70)
    print("üöÄ –ü–∞—Ä—Å–µ—Ä —Ü–µ–Ω MacBook —Å –ê–≤–∏—Ç–æ (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∞–±–ª–∏—Ü—ã URL)")
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {pages_count}")
    print(f"‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏: {PAGE_DELAY_MIN:.0f}-{PAGE_DELAY_MAX:.0f}—Å")
    print(f"‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏: {CONFIG_DELAY_MIN:.0f}-{CONFIG_DELAY_MAX:.0f}—Å")
    print("=" * 70)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É URL
    config = load_urls_config()
    entries = config.get("entries", [])
    
    if not entries:
        print("\n‚ùå –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞!")
        print(f"   –î–æ–±–∞–≤—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª: {URLS_FILE}")
        
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "generated_at": datetime.now().isoformat(),
            "total_listings": 0,
            "models": [],
            "stats": []
        }
        
        OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        return
    
    print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ {len(entries)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
    estimated_time = len(entries) * (pages_count * (PAGE_DELAY_MIN + PAGE_DELAY_MAX) / 2 + (CONFIG_DELAY_MIN + CONFIG_DELAY_MAX) / 2)
    print(f"‚è±Ô∏è –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: {estimated_time / 60:.0f} –º–∏–Ω—É—Ç")

    # –ü—Ä–æ–≥—Ä–µ–≤ —Å–µ—Å—Å–∏–∏ (cookies). –ï—Å–ª–∏ 429 –ø—Ä–∏—Ö–æ–¥–∏—Ç —Å—Ä–∞–∑—É ‚Äî –¥–∞–ª—å—à–µ –≤ GitHub Actions –æ–±—ã—á–Ω–æ –Ω–µ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª–∞ –∂–¥–∞—Ç—å.
    if not warm_up_avito():
        raise SystemExit(2)

    # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    stats = []
    total_listings = 0
    
    for i, entry in enumerate(entries, 1):
        print(f"\n\n{'#'*70}")
        print(f"# [{i}/{len(entries)}] –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
        print(f"{'#'*70}")
        
        stat = parse_entry(entry, pages_count=pages_count)
        if stat:
            stats.append(asdict(stat))
            total_listings += stat.samples_count
        
        # –ë–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏ (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π)
        if i < len(entries):
            delay = random.uniform(CONFIG_DELAY_MIN, CONFIG_DELAY_MAX)
            print(f"\n‚è∏Ô∏è –ü–∞—É–∑–∞ {delay:.0f}—Å –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π...")
            time.sleep(delay)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
    unique_models = sorted(set(s["model_name"] for s in stats))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = {
        "generated_at": datetime.now().isoformat(),
        "total_listings": total_listings,
        "models": unique_models,
        "stats": stats
    }
    
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print("\n\n" + "=" * 70)
    print("‚úÖ –ì–û–¢–û–í–û!")
    print("=" * 70)
    print(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {len(stats)}/{len(entries)}")
    print(f"   üìà –í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {total_listings:,}")
    print(f"   üè∑Ô∏è –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {len(unique_models)}")
    print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {OUTPUT_FILE}")
    print("=" * 70)


if __name__ == "__main__":
    main()
