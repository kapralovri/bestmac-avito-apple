#!/usr/bin/env python3
import json
import os
import re
import time
import random
import statistics
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict

import requests
from bs4 import BeautifulSoup

# –ü—É—Ç–∏
SCRIPT_DIR = Path(__file__).parent
URLS_FILE = SCRIPT_DIR / "../../public/data/avito-urls.json"
OUTPUT_FILE = SCRIPT_DIR / "../../public/data/avito-prices.json"

# –ù–ê–°–¢–†–û–ô–ö–ò –ó–ê–î–ï–†–ñ–ï–ö (–£—Å–∫–æ—Ä–∏–ª –≤ 2 —Ä–∞–∑–∞)
PAGE_DELAY = (5.0, 10.0)
CONFIG_DELAY = (15.0, 30.0)

@dataclass
class PriceStat:
    model_name: str
    processor: str
    ram: int
    ssd: int
    median_price: int # –≠—Ç–æ –±—É–¥–µ—Ç –Ω–∞—à "–ù–∏–∑ —Ä—ã–Ω–∫–∞"
    buyout_price: int
    samples_count: int
    updated_at: str

def get_market_low_price(prices: list[int]) -> int:
    """–ê–ª–≥–æ—Ä–∏—Ç–º '–ù–∏–∂–Ω–µ–π –ü–ª–æ—Ç–Ω–æ—Å—Ç–∏'"""
    if not prices: return 0
    prices = sorted(prices)
    n = len(prices)
    
    if n < 5: return int(statistics.median(prices))
    
    # 1. –û—Ç—Å–µ–∫–∞–µ–º —è–≤–Ω—ã–π –º—É—Å–æ—Ä (–Ω–∏–∂–Ω–∏–µ 10%)
    valid_prices = prices[int(n*0.1):]
    
    # 2. –ë–µ—Ä–µ–º 25-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å (—ç—Ç–æ –∏ –µ—Å—Ç—å –Ω–∞—á–∞–ª–æ —Ä–µ–∞–ª—å–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –¥–µ—à–µ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
    # –≠—Ç–æ —Ç–æ, —á—Ç–æ —Ç—ã –¥–µ–ª–∞–µ—à—å –≤—Ä—É—á–Ω—É—é, –∫–æ–≥–¥–∞ '—Å–º–æ—Ç—Ä–∏—à—å, –≥–¥–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å'
    idx = int(len(valid_prices) * 0.20)
    market_low = valid_prices[idx]
    
    return int(market_low)

def parse_config(entry: dict):
    model = entry['model_name']
    url = entry['url']
    print(f"üîé –ü–∞—Ä—Å–∏–º: {model} {entry['ram']}/{entry['ssd']}")
    
    prices = []
    # –ü–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Å–∞–º–æ–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ)
    for page in range(1, 3):
        try:
            time.sleep(random.uniform(*PAGE_DELAY))
            resp = requests.get(url + f"&p={page}", timeout=20)
            if resp.status_code != 200: break
            
            soup = BeautifulSoup(resp.text, 'lxml')
            items = soup.select('[data-marker="item"]')
            for item in items:
                p_text = item.select_one('[itemprop="price"]')
                if p_text:
                    p = int(re.sub(r'\D', '', p_text.get('content') or p_text.text))
                    if 20000 < p < 700000: prices.append(p)
            if len(items) < 10: break
        except: break
            
    if not prices: return None
    
    low_market = get_market_low_price(prices)
    # –¢–≤–æ—è —Ñ–æ—Ä–º—É–ª–∞: –ù–∏–∑ —Ä—ã–Ω–∫–∞ - 12 000 —Ä—É–± (—Å—Ä–µ–¥–Ω–µ–µ –æ—Ç 10-15–∫)
    buyout = int((low_market - 12000) // 1000 * 1000)
    
    return PriceStat(
        model_name=model, processor=entry['processor'],
        ram=entry['ram'], ssd=entry['ssd'],
        median_price=low_market, buyout_price=buyout,
        samples_count=len(prices), updated_at=datetime.now().isoformat()
    )

def main():
    with open(URLS_FILE, 'r') as f: config = json.load(f)
    
    # –ß—Ç–æ–±—ã –Ω–µ –ø–∞—Ä—Å–∏—Ç—å –≤—Å—ë 4 —á–∞—Å–∞, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä –±–∞—Ç—á–∞ —á–µ—Ä–µ–∑ ENV
    batch_idx = int(os.environ.get("BATCH", 1))
    total_batches = int(os.environ.get("TOTAL_BATCHES", 1))
    
    entries = config['entries']
    chunk = len(entries) // total_batches
    current_entries = entries[(batch_idx-1)*chunk : batch_idx*chunk]
    
    stats = []
    for entry in current_entries:
        stat = parse_config(entry)
        if stat:
            stats.append(asdict(stat))
            time.sleep(random.uniform(*CONFIG_DELAY))
            
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º (–º–µ—Ä–∂–∏–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏)
    old_data = {"stats": []}
    if OUTPUT_FILE.exists():
        with open(OUTPUT_FILE, 'r') as f: old_data = json.load(f)
        
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –Ω–æ–≤—ã–º–∏
    new_keys = {(s['model_name'], s['ram'], s['ssd']) for s in stats}
    final_stats = stats + [s for s in old_data['stats'] if (s['model_name'], s['ram'], s['ssd']) not in new_keys]
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump({"updated_at": datetime.now().isoformat(), "stats": final_stats}, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ –ë–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {len(final_stats)}")

if __name__ == "__main__":
    main()
